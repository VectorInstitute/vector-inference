{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vector Inference: Easy inference on Slurm clusters","text":"<p>This repository provides an easy-to-use solution to run inference servers on Slurm-managed computing clusters using vLLM. All scripts in this repository runs natively on the Vector Institute cluster environment. To adapt to other environments, update the environment variables in <code>vec_inf/client/_vars.py</code>, <code>vec_inf/client/_config.py</code>, <code>vllm.slurm</code>, <code>multinode_vllm.slurm</code> and <code>models.yaml</code> accordingly.</p>"},{"location":"#installation","title":"Installation","text":"<p>If you are using the Vector cluster environment, and you don't need any customization to the inference server environment, run the following to install package:</p> <pre><code>pip install vec-inf\n</code></pre> <p>Otherwise, we recommend using the provided <code>Dockerfile</code> to set up your own environment with the package.</p>"},{"location":"api/","title":"Python API Reference","text":"<p>This section documents the Python API for vector-inference.</p>"},{"location":"api/#client-interface","title":"Client Interface","text":""},{"location":"api/#vec_inf.client.api.VecInfClient","title":"vec_inf.client.api.VecInfClient","text":"<p>Client for interacting with Vector Inference programmatically.</p> <p>This class provides methods for launching models, checking their status, retrieving metrics, and shutting down models using the Vector Inference infrastructure.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from vec_inf.api import VecInfClient\n&gt;&gt;&gt; client = VecInfClient()\n&gt;&gt;&gt; response = client.launch_model(\"Meta-Llama-3.1-8B-Instruct\")\n&gt;&gt;&gt; job_id = response.slurm_job_id\n&gt;&gt;&gt; status = client.get_status(job_id)\n&gt;&gt;&gt; if status.status == ModelStatus.READY:\n...     print(f\"Model is ready at {status.base_url}\")\n&gt;&gt;&gt; client.shutdown_model(job_id)\n</code></pre> Source code in <code>vec_inf/client/api.py</code> <pre><code>class VecInfClient:\n    \"\"\"Client for interacting with Vector Inference programmatically.\n\n    This class provides methods for launching models, checking their status,\n    retrieving metrics, and shutting down models using the Vector Inference\n    infrastructure.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from vec_inf.api import VecInfClient\n    &gt;&gt;&gt; client = VecInfClient()\n    &gt;&gt;&gt; response = client.launch_model(\"Meta-Llama-3.1-8B-Instruct\")\n    &gt;&gt;&gt; job_id = response.slurm_job_id\n    &gt;&gt;&gt; status = client.get_status(job_id)\n    &gt;&gt;&gt; if status.status == ModelStatus.READY:\n    ...     print(f\"Model is ready at {status.base_url}\")\n    &gt;&gt;&gt; client.shutdown_model(job_id)\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the Vector Inference client.\"\"\"\n        pass\n\n    def list_models(self) -&gt; list[ModelInfo]:\n        \"\"\"List all available models.\n\n        Returns\n        -------\n        list[ModelInfo]\n            ModelInfo objects containing information about available models.\n        \"\"\"\n        model_registry = ModelRegistry()\n        return model_registry.get_all_models()\n\n    def get_model_config(self, model_name: str) -&gt; ModelConfig:\n        \"\"\"Get the configuration for a specific model.\n\n        Parameters\n        ----------\n        model_name: str\n            Name of the model to get configuration for.\n\n        Returns\n        -------\n        ModelConfig\n            Model configuration.\n        \"\"\"\n        model_registry = ModelRegistry()\n        return model_registry.get_single_model_config(model_name)\n\n    def launch_model(\n        self, model_name: str, options: Optional[LaunchOptions] = None\n    ) -&gt; LaunchResponse:\n        \"\"\"Launch a model on the cluster.\n\n        Parameters\n        ----------\n        model_name: str\n            Name of the model to launch.\n        options: LaunchOptions, optional\n            Optional launch options to override default configuration.\n\n        Returns\n        -------\n        LaunchResponse\n            Information about the launched model.\n        \"\"\"\n        # Convert LaunchOptions to dictionary if provided\n        options_dict: dict[str, Any] = {}\n        if options:\n            options_dict = {k: v for k, v in vars(options).items() if v is not None}\n\n        # Create and use the API Launch Helper\n        model_launcher = ModelLauncher(model_name, options_dict)\n        return model_launcher.launch()\n\n    def get_status(\n        self, slurm_job_id: int, log_dir: Optional[str] = None\n    ) -&gt; StatusResponse:\n        \"\"\"Get the status of a running model.\n\n        Parameters\n        ----------\n        slurm_job_id: str\n            The Slurm job ID to check.\n        log_dir: str, optional\n            Optional path to the Slurm log directory.\n\n        Returns\n        -------\n        StatusResponse\n            Model status information.\n        \"\"\"\n        model_status_monitor = ModelStatusMonitor(slurm_job_id, log_dir)\n        return model_status_monitor.process_model_status()\n\n    def get_metrics(\n        self, slurm_job_id: int, log_dir: Optional[str] = None\n    ) -&gt; MetricsResponse:\n        \"\"\"Get the performance metrics of a running model.\n\n        Parameters\n        ----------\n        slurm_job_id : str\n            The Slurm job ID to get metrics for.\n        log_dir : str, optional\n            Optional path to the Slurm log directory.\n\n        Returns\n        -------\n        MetricsResponse\n            Object containing the model's performance metrics.\n        \"\"\"\n        performance_metrics_collector = PerformanceMetricsCollector(\n            slurm_job_id, log_dir\n        )\n\n        metrics: Union[dict[str, float], str]\n        if not performance_metrics_collector.metrics_url.startswith(\"http\"):\n            metrics = performance_metrics_collector.metrics_url\n        else:\n            metrics = performance_metrics_collector.fetch_metrics()\n\n        return MetricsResponse(\n            model_name=performance_metrics_collector.status_info.model_name,\n            metrics=metrics,\n            timestamp=time.time(),\n        )\n\n    def shutdown_model(self, slurm_job_id: int) -&gt; bool:\n        \"\"\"Shutdown a running model.\n\n        Parameters\n        ----------\n        slurm_job_id: str\n            The Slurm job ID to shut down.\n\n        Returns\n        -------\n        bool\n            True if the model was successfully shutdown, False otherwise.\n\n        Raises\n        ------\n        SlurmJobError\n            If there was an error shutting down the model.\n        \"\"\"\n        shutdown_cmd = f\"scancel {slurm_job_id}\"\n        _, stderr = run_bash_command(shutdown_cmd)\n        if stderr:\n            raise SlurmJobError(f\"Failed to shutdown model: {stderr}\")\n        return True\n\n    def wait_until_ready(\n        self,\n        slurm_job_id: int,\n        timeout_seconds: int = 1800,\n        poll_interval_seconds: int = 10,\n        log_dir: Optional[str] = None,\n    ) -&gt; StatusResponse:\n        \"\"\"Wait until a model is ready or fails.\n\n        Parameters\n        ----------\n        slurm_job_id: str\n            The Slurm job ID to wait for.\n        timeout_seconds: int\n            Maximum time to wait in seconds (default: 30 mins).\n        poll_interval_seconds: int\n            How often to check status in seconds (default: 10s).\n        log_dir: str, optional\n            Optional path to the Slurm log directory.\n\n        Returns\n        -------\n        StatusResponse\n            Status, if the model is ready or failed.\n\n        Raises\n        ------\n        SlurmJobError\n            If the specified job is not found or there's an error with the job.\n        ServerError\n            If the server fails to start within the timeout period.\n        APIError\n            If there was an error checking the status.\n\n        \"\"\"\n        start_time = time.time()\n\n        while True:\n            status_info = self.get_status(slurm_job_id, log_dir)\n\n            if status_info.server_status == ModelStatus.READY:\n                return status_info\n\n            if status_info.server_status == ModelStatus.FAILED:\n                error_message = status_info.failed_reason or \"Unknown error\"\n                raise ServerError(f\"Model failed to start: {error_message}\")\n\n            if status_info.server_status == ModelStatus.SHUTDOWN:\n                raise ServerError(\"Model was shutdown before it became ready\")\n\n            # Check timeout\n            if time.time() - start_time &gt; timeout_seconds:\n                raise ServerError(\n                    f\"Timed out waiting for model to become ready after {timeout_seconds} seconds\"\n                )\n\n            # Wait before checking again\n            time.sleep(poll_interval_seconds)\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the Vector Inference client.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the Vector Inference client.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.list_models","title":"list_models","text":"<pre><code>list_models()\n</code></pre> <p>List all available models.</p> <p>Returns:</p> Type Description <code>list[ModelInfo]</code> <p>ModelInfo objects containing information about available models.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def list_models(self) -&gt; list[ModelInfo]:\n    \"\"\"List all available models.\n\n    Returns\n    -------\n    list[ModelInfo]\n        ModelInfo objects containing information about available models.\n    \"\"\"\n    model_registry = ModelRegistry()\n    return model_registry.get_all_models()\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.get_model_config","title":"get_model_config","text":"<pre><code>get_model_config(model_name)\n</code></pre> <p>Get the configuration for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to get configuration for.</p> required <p>Returns:</p> Type Description <code>ModelConfig</code> <p>Model configuration.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def get_model_config(self, model_name: str) -&gt; ModelConfig:\n    \"\"\"Get the configuration for a specific model.\n\n    Parameters\n    ----------\n    model_name: str\n        Name of the model to get configuration for.\n\n    Returns\n    -------\n    ModelConfig\n        Model configuration.\n    \"\"\"\n    model_registry = ModelRegistry()\n    return model_registry.get_single_model_config(model_name)\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.launch_model","title":"launch_model","text":"<pre><code>launch_model(model_name, options=None)\n</code></pre> <p>Launch a model on the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to launch.</p> required <code>options</code> <code>Optional[LaunchOptions]</code> <p>Optional launch options to override default configuration.</p> <code>None</code> <p>Returns:</p> Type Description <code>LaunchResponse</code> <p>Information about the launched model.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def launch_model(\n    self, model_name: str, options: Optional[LaunchOptions] = None\n) -&gt; LaunchResponse:\n    \"\"\"Launch a model on the cluster.\n\n    Parameters\n    ----------\n    model_name: str\n        Name of the model to launch.\n    options: LaunchOptions, optional\n        Optional launch options to override default configuration.\n\n    Returns\n    -------\n    LaunchResponse\n        Information about the launched model.\n    \"\"\"\n    # Convert LaunchOptions to dictionary if provided\n    options_dict: dict[str, Any] = {}\n    if options:\n        options_dict = {k: v for k, v in vars(options).items() if v is not None}\n\n    # Create and use the API Launch Helper\n    model_launcher = ModelLauncher(model_name, options_dict)\n    return model_launcher.launch()\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.get_status","title":"get_status","text":"<pre><code>get_status(slurm_job_id, log_dir=None)\n</code></pre> <p>Get the status of a running model.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>int</code> <p>The Slurm job ID to check.</p> required <code>log_dir</code> <code>Optional[str]</code> <p>Optional path to the Slurm log directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>StatusResponse</code> <p>Model status information.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def get_status(\n    self, slurm_job_id: int, log_dir: Optional[str] = None\n) -&gt; StatusResponse:\n    \"\"\"Get the status of a running model.\n\n    Parameters\n    ----------\n    slurm_job_id: str\n        The Slurm job ID to check.\n    log_dir: str, optional\n        Optional path to the Slurm log directory.\n\n    Returns\n    -------\n    StatusResponse\n        Model status information.\n    \"\"\"\n    model_status_monitor = ModelStatusMonitor(slurm_job_id, log_dir)\n    return model_status_monitor.process_model_status()\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics(slurm_job_id, log_dir=None)\n</code></pre> <p>Get the performance metrics of a running model.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>str</code> <p>The Slurm job ID to get metrics for.</p> required <code>log_dir</code> <code>str</code> <p>Optional path to the Slurm log directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>MetricsResponse</code> <p>Object containing the model's performance metrics.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def get_metrics(\n    self, slurm_job_id: int, log_dir: Optional[str] = None\n) -&gt; MetricsResponse:\n    \"\"\"Get the performance metrics of a running model.\n\n    Parameters\n    ----------\n    slurm_job_id : str\n        The Slurm job ID to get metrics for.\n    log_dir : str, optional\n        Optional path to the Slurm log directory.\n\n    Returns\n    -------\n    MetricsResponse\n        Object containing the model's performance metrics.\n    \"\"\"\n    performance_metrics_collector = PerformanceMetricsCollector(\n        slurm_job_id, log_dir\n    )\n\n    metrics: Union[dict[str, float], str]\n    if not performance_metrics_collector.metrics_url.startswith(\"http\"):\n        metrics = performance_metrics_collector.metrics_url\n    else:\n        metrics = performance_metrics_collector.fetch_metrics()\n\n    return MetricsResponse(\n        model_name=performance_metrics_collector.status_info.model_name,\n        metrics=metrics,\n        timestamp=time.time(),\n    )\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.shutdown_model","title":"shutdown_model","text":"<pre><code>shutdown_model(slurm_job_id)\n</code></pre> <p>Shutdown a running model.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>int</code> <p>The Slurm job ID to shut down.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the model was successfully shutdown, False otherwise.</p> <p>Raises:</p> Type Description <code>SlurmJobError</code> <p>If there was an error shutting down the model.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def shutdown_model(self, slurm_job_id: int) -&gt; bool:\n    \"\"\"Shutdown a running model.\n\n    Parameters\n    ----------\n    slurm_job_id: str\n        The Slurm job ID to shut down.\n\n    Returns\n    -------\n    bool\n        True if the model was successfully shutdown, False otherwise.\n\n    Raises\n    ------\n    SlurmJobError\n        If there was an error shutting down the model.\n    \"\"\"\n    shutdown_cmd = f\"scancel {slurm_job_id}\"\n    _, stderr = run_bash_command(shutdown_cmd)\n    if stderr:\n        raise SlurmJobError(f\"Failed to shutdown model: {stderr}\")\n    return True\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.wait_until_ready","title":"wait_until_ready","text":"<pre><code>wait_until_ready(\n    slurm_job_id,\n    timeout_seconds=1800,\n    poll_interval_seconds=10,\n    log_dir=None,\n)\n</code></pre> <p>Wait until a model is ready or fails.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>int</code> <p>The Slurm job ID to wait for.</p> required <code>timeout_seconds</code> <code>int</code> <p>Maximum time to wait in seconds (default: 30 mins).</p> <code>1800</code> <code>poll_interval_seconds</code> <code>int</code> <p>How often to check status in seconds (default: 10s).</p> <code>10</code> <code>log_dir</code> <code>Optional[str]</code> <p>Optional path to the Slurm log directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>StatusResponse</code> <p>Status, if the model is ready or failed.</p> <p>Raises:</p> Type Description <code>SlurmJobError</code> <p>If the specified job is not found or there's an error with the job.</p> <code>ServerError</code> <p>If the server fails to start within the timeout period.</p> <code>APIError</code> <p>If there was an error checking the status.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def wait_until_ready(\n    self,\n    slurm_job_id: int,\n    timeout_seconds: int = 1800,\n    poll_interval_seconds: int = 10,\n    log_dir: Optional[str] = None,\n) -&gt; StatusResponse:\n    \"\"\"Wait until a model is ready or fails.\n\n    Parameters\n    ----------\n    slurm_job_id: str\n        The Slurm job ID to wait for.\n    timeout_seconds: int\n        Maximum time to wait in seconds (default: 30 mins).\n    poll_interval_seconds: int\n        How often to check status in seconds (default: 10s).\n    log_dir: str, optional\n        Optional path to the Slurm log directory.\n\n    Returns\n    -------\n    StatusResponse\n        Status, if the model is ready or failed.\n\n    Raises\n    ------\n    SlurmJobError\n        If the specified job is not found or there's an error with the job.\n    ServerError\n        If the server fails to start within the timeout period.\n    APIError\n        If there was an error checking the status.\n\n    \"\"\"\n    start_time = time.time()\n\n    while True:\n        status_info = self.get_status(slurm_job_id, log_dir)\n\n        if status_info.server_status == ModelStatus.READY:\n            return status_info\n\n        if status_info.server_status == ModelStatus.FAILED:\n            error_message = status_info.failed_reason or \"Unknown error\"\n            raise ServerError(f\"Model failed to start: {error_message}\")\n\n        if status_info.server_status == ModelStatus.SHUTDOWN:\n            raise ServerError(\"Model was shutdown before it became ready\")\n\n        # Check timeout\n        if time.time() - start_time &gt; timeout_seconds:\n            raise ServerError(\n                f\"Timed out waiting for model to become ready after {timeout_seconds} seconds\"\n            )\n\n        # Wait before checking again\n        time.sleep(poll_interval_seconds)\n</code></pre>"},{"location":"api/#data-models","title":"Data Models","text":""},{"location":"api/#vec_inf.client._models","title":"vec_inf.client._models","text":"<p>Data models for Vector Inference API.</p> <p>This module contains the data model classes used by the Vector Inference API for both request parameters and response objects.</p>"},{"location":"api/#vec_inf.client._models.ModelStatus","title":"ModelStatus","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum representing the possible status states of a model.</p> Source code in <code>vec_inf/client/_models.py</code> <pre><code>class ModelStatus(str, Enum):\n    \"\"\"Enum representing the possible status states of a model.\"\"\"\n\n    PENDING = \"PENDING\"\n    LAUNCHING = \"LAUNCHING\"\n    READY = \"READY\"\n    FAILED = \"FAILED\"\n    SHUTDOWN = \"SHUTDOWN\"\n    UNAVAILABLE = \"UNAVAILABLE\"\n</code></pre>"},{"location":"api/#vec_inf.client._models.ModelType","title":"ModelType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum representing the possible model types.</p> Source code in <code>vec_inf/client/_models.py</code> <pre><code>class ModelType(str, Enum):\n    \"\"\"Enum representing the possible model types.\"\"\"\n\n    LLM = \"LLM\"\n    VLM = \"VLM\"\n    TEXT_EMBEDDING = \"Text_Embedding\"\n    REWARD_MODELING = \"Reward_Modeling\"\n</code></pre>"},{"location":"api/#vec_inf.client._models.LaunchResponse","title":"LaunchResponse  <code>dataclass</code>","text":"<p>Response from launching a model.</p> Source code in <code>vec_inf/client/_models.py</code> <pre><code>@dataclass\nclass LaunchResponse:\n    \"\"\"Response from launching a model.\"\"\"\n\n    slurm_job_id: int\n    model_name: str\n    config: dict[str, Any]\n    raw_output: str = field(repr=False)\n</code></pre>"},{"location":"api/#vec_inf.client._models.StatusResponse","title":"StatusResponse  <code>dataclass</code>","text":"<p>Response from checking a model's status.</p> Source code in <code>vec_inf/client/_models.py</code> <pre><code>@dataclass\nclass StatusResponse:\n    \"\"\"Response from checking a model's status.\"\"\"\n\n    model_name: str\n    server_status: ModelStatus\n    job_state: Union[str, ModelStatus]\n    raw_output: str = field(repr=False)\n    base_url: Optional[str] = None\n    pending_reason: Optional[str] = None\n    failed_reason: Optional[str] = None\n</code></pre>"},{"location":"api/#vec_inf.client._models.MetricsResponse","title":"MetricsResponse  <code>dataclass</code>","text":"<p>Response from retrieving model metrics.</p> Source code in <code>vec_inf/client/_models.py</code> <pre><code>@dataclass\nclass MetricsResponse:\n    \"\"\"Response from retrieving model metrics.\"\"\"\n\n    model_name: str\n    metrics: Union[dict[str, float], str]\n    timestamp: float\n</code></pre>"},{"location":"api/#vec_inf.client._models.LaunchOptions","title":"LaunchOptions  <code>dataclass</code>","text":"<p>Options for launching a model.</p> Source code in <code>vec_inf/client/_models.py</code> <pre><code>@dataclass\nclass LaunchOptions:\n    \"\"\"Options for launching a model.\"\"\"\n\n    model_family: Optional[str] = None\n    model_variant: Optional[str] = None\n    max_model_len: Optional[int] = None\n    max_num_seqs: Optional[int] = None\n    gpu_memory_utilization: Optional[float] = None\n    enable_prefix_caching: Optional[bool] = None\n    enable_chunked_prefill: Optional[bool] = None\n    max_num_batched_tokens: Optional[int] = None\n    partition: Optional[str] = None\n    num_nodes: Optional[int] = None\n    gpus_per_node: Optional[int] = None\n    qos: Optional[str] = None\n    time: Optional[str] = None\n    vocab_size: Optional[int] = None\n    data_type: Optional[str] = None\n    venv: Optional[str] = None\n    log_dir: Optional[str] = None\n    model_weights_parent_dir: Optional[str] = None\n    pipeline_parallelism: Optional[bool] = None\n    compilation_config: Optional[str] = None\n    enforce_eager: Optional[bool] = None\n</code></pre>"},{"location":"api/#vec_inf.client._models.LaunchOptionsDict","title":"LaunchOptionsDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for LaunchOptions.</p> Source code in <code>vec_inf/client/_models.py</code> <pre><code>class LaunchOptionsDict(TypedDict):\n    \"\"\"TypedDict for LaunchOptions.\"\"\"\n\n    model_family: NotRequired[Optional[str]]\n    model_variant: NotRequired[Optional[str]]\n    max_model_len: NotRequired[Optional[int]]\n    max_num_seqs: NotRequired[Optional[int]]\n    gpu_memory_utilization: NotRequired[Optional[float]]\n    enable_prefix_caching: NotRequired[Optional[bool]]\n    enable_chunked_prefill: NotRequired[Optional[bool]]\n    max_num_batched_tokens: NotRequired[Optional[int]]\n    partition: NotRequired[Optional[str]]\n    num_nodes: NotRequired[Optional[int]]\n    gpus_per_node: NotRequired[Optional[int]]\n    qos: NotRequired[Optional[str]]\n    time: NotRequired[Optional[str]]\n    vocab_size: NotRequired[Optional[int]]\n    data_type: NotRequired[Optional[str]]\n    venv: NotRequired[Optional[str]]\n    log_dir: NotRequired[Optional[str]]\n    model_weights_parent_dir: NotRequired[Optional[str]]\n    pipeline_parallelism: NotRequired[Optional[bool]]\n    compilation_config: NotRequired[Optional[str]]\n    enforce_eager: NotRequired[Optional[bool]]\n</code></pre>"},{"location":"api/#vec_inf.client._models.ModelInfo","title":"ModelInfo  <code>dataclass</code>","text":"<p>Information about an available model.</p> Source code in <code>vec_inf/client/_models.py</code> <pre><code>@dataclass\nclass ModelInfo:\n    \"\"\"Information about an available model.\"\"\"\n\n    name: str\n    family: str\n    variant: Optional[str]\n    type: ModelType\n    config: dict[str, Any]\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#cli-usage","title":"CLI Usage","text":""},{"location":"user_guide/#launch-command","title":"<code>launch</code> command","text":"<p>The <code>launch</code> command allows users to deploy a model as a slurm job. If the job successfully launches, a URL endpoint is exposed for the user to send requests for inference.</p> <p>We will use the Llama 3.1 model as example, to launch an OpenAI compatible inference server for Meta-Llama-3.1-8B-Instruct, run:</p> <p><pre><code>vec-inf launch Meta-Llama-3.1-8B-Instruct\n</code></pre> You should see an output like the following:</p> <p></p>"},{"location":"user_guide/#overrides","title":"Overrides","text":"<p>Models that are already supported by <code>vec-inf</code> would be launched using the cached configuration or default configuration. You can override these values by providing additional parameters. Use <code>vec-inf launch --help</code> to see the full list of parameters that can be overriden. For example, if <code>qos</code> is to be overriden:</p> <pre><code>vec-inf launch Meta-Llama-3.1-8B-Instruct --qos &lt;new_qos&gt;\n</code></pre>"},{"location":"user_guide/#custom-models","title":"Custom models","text":"<p>You can also launch your own custom model as long as the model architecture is supported by vLLM, and make sure to follow the instructions below: * Your model weights directory naming convention should follow <code>$MODEL_FAMILY-$MODEL_VARIANT</code> ($MODEL_VARIANT is OPTIONAL). * Your model weights directory should contain HuggingFace format weights. * You should specify your model configuration by:   * Creating a custom configuration file for your model and specify its path via setting the environment variable <code>VEC_INF_CONFIG</code>. Check the default parameters file for the format of the config file. All the parameters for the model should be specified in that config file.   * Using launch command options to specify your model setup. * For other model launch parameters you can reference the default values for similar models using the <code>list</code> command .</p> <p>Here is an example to deploy a custom Qwen2.5-7B-Instruct-1M model which is not supported in the default list of models using a user custom config. In this case, the model weights are assumed to be downloaded to a <code>model-weights</code> directory inside the user's home directory. The weights directory of the model follows the naming convention so it would be named <code>Qwen2.5-7B-Instruct-1M</code>. The following yaml file would need to be created, lets say it is named <code>/h/&lt;username&gt;/my-model-config.yaml</code>.</p> <pre><code>models:\n  Qwen2.5-7B-Instruct-1M:\n    model_family: Qwen2.5\n    model_variant: 7B-Instruct-1M\n    model_type: LLM\n    gpus_per_node: 1\n    num_nodes: 1\n    vocab_size: 152064\n    max_model_len: 1010000\n    max_num_seqs: 256\n    pipeline_parallelism: true\n    enforce_eager: false\n    qos: m2\n    time: 08:00:00\n    partition: a40\n    model_weights_parent_dir: /h/&lt;username&gt;/model-weights\n</code></pre> <p>You would then set the <code>VEC_INF_CONFIG</code> path using:</p> <pre><code>export VEC_INF_CONFIG=/h/&lt;username&gt;/my-model-config.yaml\n</code></pre> <p>Note that there are other parameters that can also be added to the config but not shown in this example, such as <code>data_type</code> and <code>log_dir</code>.</p>"},{"location":"user_guide/#status-command","title":"<code>status</code> command","text":"<p>You can check the inference server status by providing the Slurm job ID to the <code>status</code> command:</p> <pre><code>vec-inf status 15373800\n</code></pre> <p>If the server is pending for resources, you should see an output like this:</p> <p></p> <p>When the server is ready, you should see an output like this:</p> <p></p> <p>There are 5 possible states:</p> <ul> <li>PENDING: Job submitted to Slurm, but not executed yet. Job pending reason will be shown.</li> <li>LAUNCHING: Job is running but the server is not ready yet.</li> <li>READY: Inference server running and ready to take requests.</li> <li>FAILED: Inference server in an unhealthy state. Job failed reason will be shown.</li> <li>SHUTDOWN: Inference server is shutdown/cancelled.</li> </ul> <p>Note that the base URL is only available when model is in <code>READY</code> state, and if you've changed the Slurm log directory path, you also need to specify it when using the <code>status</code> command.</p>"},{"location":"user_guide/#metrics-command","title":"<code>metrics</code> command","text":"<p>Once your server is ready, you can check performance metrics by providing the Slurm job ID to the <code>metrics</code> command: <pre><code>vec-inf metrics 15373800\n</code></pre></p> <p>And you will see the performance metrics streamed to your console, note that the metrics are updated with a 2-second interval.</p> <p></p>"},{"location":"user_guide/#shutdown-command","title":"<code>shutdown</code> command","text":"<p>Finally, when you're finished using a model, you can shut it down by providing the Slurm job ID: <pre><code>vec-inf shutdown 15373800\n\n&gt; Shutting down model with Slurm Job ID: 15373800\n</code></pre></p>"},{"location":"user_guide/#list-command","title":"<code>list</code> command","text":"<p>You call view the full list of available models by running the <code>list</code> command:</p> <pre><code>vec-inf list\n</code></pre> <p></p> <p>You can also view the default setup for a specific supported model by providing the model name, for example <code>Meta-Llama-3.1-70B-Instruct</code>:</p> <pre><code>vec-inf list Meta-Llama-3.1-70B-Instruct\n</code></pre> <p></p> <p><code>launch</code>, <code>list</code>, and <code>status</code> command supports <code>--json-mode</code>, where the command output would be structured as a JSON string.</p>"},{"location":"user_guide/#send-inference-requests","title":"Send inference requests","text":"<p>Once the inference server is ready, you can start sending in inference requests. We provide example scripts for sending inference requests in <code>examples</code> folder. Make sure to update the model server URL and the model weights location in the scripts. For example, you can run <code>python examples/inference/llm/chat_completions.py</code>, and you should expect to see an output like the following:</p> <pre><code>{\n    \"id\":\"chatcmpl-387c2579231948ffaf66cdda5439d3dc\",\n    \"choices\": [\n        {\n            \"finish_reason\":\"stop\",\n            \"index\":0,\n            \"logprobs\":null,\n            \"message\": {\n                \"content\":\"Arrr, I be Captain Chatbeard, the scurviest chatbot on the seven seas! Ye be wantin' to know me identity, eh? Well, matey, I be a swashbucklin' AI, here to provide ye with answers and swappin' tales, savvy?\",\n                \"role\":\"assistant\",\n                \"function_call\":null,\n                \"tool_calls\":[],\n                \"reasoning_content\":null\n            },\n            \"stop_reason\":null\n        }\n    ],\n    \"created\":1742496683,\n    \"model\":\"Meta-Llama-3.1-8B-Instruct\",\n    \"object\":\"chat.completion\",\n    \"system_fingerprint\":null,\n    \"usage\": {\n        \"completion_tokens\":66,\n        \"prompt_tokens\":32,\n        \"total_tokens\":98,\n        \"prompt_tokens_details\":null\n    },\n    \"prompt_logprobs\":null\n}\n</code></pre> <p>NOTE: For multimodal models, currently only <code>ChatCompletion</code> is available, and only one image can be provided for each prompt.</p>"},{"location":"user_guide/#ssh-tunnel-from-your-local-device","title":"SSH tunnel from your local device","text":"<p>If you want to run inference from your local device, you can open a SSH tunnel to your cluster environment like the following: <pre><code>ssh -L 8081:172.17.8.29:8081 username@v.vectorinstitute.ai -N\n</code></pre> Where the last number in the URL is the GPU number (gpu029 in this case). The example provided above is for the vector cluster, change the variables accordingly for your environment</p>"},{"location":"user_guide/#python-api-usage","title":"Python API Usage","text":"<p>You can also use the <code>vec_inf</code> Python API to launch and manage inference servers.</p> <p>Check out the Python API documentation for more details. There are also Python API usage examples in the <code>examples/api</code> folder.</p>"}]}