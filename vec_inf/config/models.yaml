models:
  c4ai-command-r-plus:
    model_family: c4ai-command-r
    model_variant: plus
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 256000
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  c4ai-command-r-plus-08-2024:
    model_family: c4ai-command-r
    model_variant: plus-08-2024
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 256000
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  c4ai-command-r-08-2024:
    model_family: c4ai-command-r
    model_variant: 08-2024
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  CodeLlama-7b-hf:
    model_family: CodeLlama
    model_variant: 7b-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  CodeLlama-7b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 7b-Instruct-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  CodeLlama-13b-hf:
    model_family: CodeLlama
    model_variant: 13b-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  CodeLlama-13b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 13b-Instruct-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  CodeLlama-34b-hf:
    model_family: CodeLlama
    model_variant: 34b-hf
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  CodeLlama-34b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 34b-Instruct-hf
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  CodeLlama-70b-hf:
    model_family: CodeLlama
    model_variant: 70b-hf
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  CodeLlama-70b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 70b-Instruct-hf
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  dbrx-instruct:
    model_family: dbrx
    model_variant: instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 100352
    max_model_len: 32000
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  gemma-2-9b:
    model_family: gemma-2
    model_variant: 9b
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  gemma-2-9b-it:
    model_family: gemma-2
    model_variant: 9b-it
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  gemma-2-27b:
    model_family: gemma-2
    model_variant: 27b
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  gemma-2-27b-it:
    model_family: gemma-2
    model_variant: 27b-it
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-2-7b-hf:
    model_family: Llama-2
    model_variant: 7b-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-2-7b-chat-hf:
    model_family: Llama-2
    model_variant: 7b-chat-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-2-13b-hf:
    model_family: Llama-2
    model_variant: 13b-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-2-13b-chat-hf:
    model_family: Llama-2
    model_variant: 13b-chat-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-2-70b-hf:
    model_family: Llama-2
    model_variant: 70b-hf
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-2-70b-chat-hf:
    model_family: Llama-2
    model_variant: 70b-chat-hf
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  llava-1.5-7b-hf:
    model_family: llava-1.5
    model_variant: 7b-hf
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  llava-1.5-13b-hf:
    model_family: llava-1.5
    model_variant: 13b-hf
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  llava-v1.6-mistral-7b-hf:
    model_family: llava-v1.6
    model_variant: mistral-7b-hf
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  llava-v1.6-34b-hf:
    model_family: llava-v1.6
    model_variant: 34b-hf
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 64064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3-8B:
    model_family: Meta-Llama-3
    model_variant: 8B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3-8B-Instruct:
    model_family: Meta-Llama-3
    model_variant: 8B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3-70B:
    model_family: Meta-Llama-3
    model_variant: 70B
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3-70B-Instruct:
    model_family: Meta-Llama-3
    model_variant: 70B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3.1-8B:
    model_family: Meta-Llama-3.1
    model_variant: 8B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3.1-8B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 8B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3.1-70B:
    model_family: Meta-Llama-3.1
    model_variant: 70B
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3.1-70B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 70B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Meta-Llama-3.1-405B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 405B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 8
    vocab_size: 128256
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m4
    time: 02:00:00
    partition: a40
  Mistral-7B-v0.1:
    model_family: Mistral
    model_variant: 7B-v0.1
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mistral-7B-Instruct-v0.1:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.1
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mistral-7B-Instruct-v0.2:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.2
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mistral-7B-v0.3:
    model_family: Mistral
    model_variant: 7B-v0.3
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32768
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mistral-7B-Instruct-v0.3:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.3
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32768
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mistral-Large-Instruct-2407:
    model_family: Mistral
    model_variant: Large-Instruct-2407
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 32768
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mistral-Large-Instruct-2411:
    model_family: Mistral
    model_variant: Large-Instruct-2411
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 32768
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mixtral-8x7B-Instruct-v0.1:
    model_family: Mixtral
    model_variant: 8x7B-Instruct-v0.1
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mixtral-8x22B-v0.1:
    model_family: Mixtral
    model_variant: 8x22B-v0.1
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 32768
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Mixtral-8x22B-Instruct-v0.1:
    model_family: Mixtral
    model_variant: 8x22B-Instruct-v0.1
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 32768
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Phi-3-medium-128k-instruct:
    model_family: Phi-3
    model_variant: medium-128k-instruct
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Phi-3-vision-128k-instruct:
    model_family: Phi-3-vision
    model_variant: 128k-instruct
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32064
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama3-OpenBioLLM-70B:
    model_family: Llama3-OpenBioLLM
    model_variant: 70B
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.1-Nemotron-70B-Instruct-HF:
    model_family: Llama-3.1-Nemotron
    model_variant: 70B-Instruct-HF
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.2-1B:
    model_family: Llama-3.2
    model_variant: 1B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.2-1B-Instruct:
    model_family: Llama-3.2
    model_variant: 1B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.2-3B:
    model_family: Llama-3.2
    model_variant: 3B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.2-3B-Instruct:
    model_family: Llama-3.2
    model_variant: 3B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.2-11B-Vision:
    model_family: Llama-3.2
    model_variant: 11B-Vision
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 4096
    max_num_seqs: 64
    pipeline_parallelism: false
    enforce_eager: true
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.2-11B-Vision-Instruct:
    model_family: Llama-3.2
    model_variant: 11B-Vision-Instruct
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 4096
    max_num_seqs: 64
    pipeline_parallelism: false
    enforce_eager: true
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.2-90B-Vision:
    model_family: Llama-3.2
    model_variant: 90B-Vision
    model_type: VLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 128256
    max_model_len: 4096
    max_num_seqs: 32
    pipeline_parallelism: false
    enforce_eager: true
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.2-90B-Vision-Instruct:
    model_family: Llama-3.2
    model_variant: 90B-Vision-Instruct
    model_type: VLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 128256
    max_model_len: 4096
    max_num_seqs: 32
    pipeline_parallelism: false
    enforce_eager: true
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-0.5B-Instruct:
    model_family: Qwen2.5
    model_variant: 0.5B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-1.5B-Instruct:
    model_family: Qwen2.5
    model_variant: 1.5B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-3B-Instruct:
    model_family: Qwen2.5
    model_variant: 3B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-7B-Instruct:
    model_family: Qwen2.5
    model_variant: 7B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-14B-Instruct:
    model_family: Qwen2.5
    model_variant: 14B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-32B-Instruct:
    model_family: Qwen2.5
    model_variant: 32B-Instruct
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-72B-Instruct:
    model_family: Qwen2.5
    model_variant: 72B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-Math-1.5B-Instruct:
    model_family: Qwen2.5
    model_variant: Math-1.5B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-Math-7B-Instruct:
    model_family: Qwen2.5
    model_variant: Math-7B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-Math-72B-Instruct:
    model_family: Qwen2.5
    model_variant: Math-72B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-Coder-7B-Instruct:
    model_family: Qwen2.5
    model_variant: Coder-7B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-Math-RM-72B:
    model_family: Qwen2.5
    model_variant: Math-RM-72B
    model_type: Reward_Modeling
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Qwen2.5-Math-PRM-7B:
    model_family: Qwen2.5
    model_variant: Math-PRM-7B
    model_type: Reward_Modeling
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  QwQ-32B-Preview:
    model_family: QwQ
    model_variant: 32B-Preview
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Pixtral-12B-2409:
    model_family: Pixtral
    model_variant: 12B-2409
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 131072
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  e5-mistral-7b-instruct:
    model_family: e5
    model_variant: mistral-7b-instruct
    model_type: Text_Embedding
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  bge-base-en-v1.5:
    model_family: bge
    model_variant: base-en-v1.5
    model_type: Text_Embedding
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 30522
    max_model_len: 512
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  all-MiniLM-L6-v2:
    model_family: all-MiniLM
    model_variant: L6-v2
    model_type: Text_Embedding
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 30522
    max_model_len: 512
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Llama-3.3-70B-Instruct:
    model_family: Llama-3.3
    model_variant: 70B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  InternVL2_5-26B:
    model_family: InternVL2_5
    model_variant: 26B
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 92553
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  InternVL2_5-38B:
    model_family: InternVL2_5
    model_variant: 38B
    model_type: VLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 92553
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Aya-Expanse-32B:
    model_family: Aya-Expanse
    model_variant: 32B
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  DeepSeek-R1-Distill-Llama-70B:
    model_family: DeepSeek-R1
    model_variant: 'Distill-Llama-70B '
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  DeepSeek-R1-Distill-Llama-8B:
    model_family: DeepSeek-R1
    model_variant: 'Distill-Llama-8B '
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  DeepSeek-R1-Distill-Qwen-32B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-32B
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  DeepSeek-R1-Distill-Qwen-14B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-14B
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  DeepSeek-R1-Distill-Qwen-7B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-7B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  DeepSeek-R1-Distill-Qwen-1.5B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-1.5B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Phi-3.5-vision-instruct:
    model_family: Phi-3.5-vision
    model_variant: instruct
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32064
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  InternVL2_5-8B:
    model_family: InternVL2_5
    model_variant: 8B
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 92553
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  glm-4v-9b:
    model_family: glm-4v
    model_variant: 9b
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 151552
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  Molmo-7B-D-0924:
    model_family: Molmo
    model_variant: 7B-D-0924
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  deepseek-vl2:
    model_family: deepseek-vl2
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 129280
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
  deepseek-vl2-small:
    model_family: deepseek-vl2
    model_variant: small
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 129280
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    qos: m2
    time: 08:00:00
    partition: a40
