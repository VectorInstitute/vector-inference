models:
  c4ai-command-r-plus-08-2024:
    model_family: c4ai-command-r
    model_variant: plus-08-2024
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 256000
    vllm_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
      --max-model-len: 65536
    sglang_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
      --context-length: 65536
  c4ai-command-r-08-2024:
    model_family: c4ai-command-r
    model_variant: 08-2024
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 256000
    vllm_args:
      --tensor-parallel-size: 2
      --max-model-len: 32768
    sglang_args:
      --tensor-parallel-size: 2
      --context-length: 32768
  CodeLlama-7b-hf:
    model_family: CodeLlama
    model_variant: 7b-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  CodeLlama-7b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 7b-Instruct-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  CodeLlama-13b-hf:
    model_family: CodeLlama
    model_variant: 13b-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  CodeLlama-13b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 13b-Instruct-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  CodeLlama-34b-hf:
    model_family: CodeLlama
    model_variant: 34b-hf
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32000
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  CodeLlama-34b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 34b-Instruct-hf
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32000
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  CodeLlama-70b-hf:
    model_family: CodeLlama
    model_variant: 70b-hf
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32016
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  CodeLlama-70b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 70b-Instruct-hf
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32016
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  gemma-2-2b-it:
    model_family: gemma-2
    model_variant: 2b-it
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 256000
  gemma-2-9b:
    model_family: gemma-2
    model_variant: 9b
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 256000
  gemma-2-9b-it:
    model_family: gemma-2
    model_variant: 9b-it
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 256000
  gemma-2-27b:
    model_family: gemma-2
    model_variant: 27b
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 256000
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  gemma-2-27b-it:
    model_family: gemma-2
    model_variant: 27b-it
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 256000
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  Llama-2-7b-hf:
    model_family: Llama-2
    model_variant: 7b-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  Llama-2-7b-chat-hf:
    model_family: Llama-2
    model_variant: 7b-chat-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  Llama-2-13b-hf:
    model_family: Llama-2
    model_variant: 13b-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  Llama-2-13b-chat-hf:
    model_family: Llama-2
    model_variant: 13b-chat-hf
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  Llama-2-70b-chat-hf:
    model_family: Llama-2
    model_variant: 70b-chat-hf
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32000
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  llava-1.5-7b-hf:
    model_family: llava-1.5
    model_variant: 7b-hf
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  llava-1.5-13b-hf:
    model_family: llava-1.5
    model_variant: 13b-hf
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  llava-v1.6-mistral-7b-hf:
    model_family: llava-v1.6
    model_variant: mistral-7b-hf
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32064
  llava-v1.6-34b-hf:
    model_family: llava-v1.6
    model_variant: 34b-hf
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 64064
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  Meta-Llama-3-8B:
    model_family: Meta-Llama-3
    model_variant: 8B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  Meta-Llama-3-8B-Instruct:
    model_family: Meta-Llama-3
    model_variant: 8B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  Meta-Llama-3-70B:
    model_family: Meta-Llama-3
    model_variant: 70B
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  Meta-Llama-3-70B-Instruct:
    model_family: Meta-Llama-3
    model_variant: 70B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  Meta-Llama-3.1-8B:
    model_family: Meta-Llama-3.1
    model_variant: 8B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  Meta-Llama-3.1-8B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 8B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  Meta-Llama-3.1-70B:
    model_family: Meta-Llama-3.1
    model_variant: 70B
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    vllm_args:
      --tensor-parallel-size: 4
      --max-model-len: 65536
    sglang_args:
      --tensor-parallel-size: 4
      --context-length: 65536
  Meta-Llama-3.1-70B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 70B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    vllm_args:
      --tensor-parallel-size: 4
      --max-model-len: 65536
    sglang_args:
      --tensor-parallel-size: 4
      --context-length: 65536
  Meta-Llama-3.1-405B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 405B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 8
    vocab_size: 128256
    vllm_args:
      --pipeline-parallel-size: 8
      --tensor-parallel-size: 4
      --max-model-len: 16384
    sglang_args:
      --pipeline-parallel-size: 8
      --tensor-parallel-size: 4
      --context-length: 16384
  Mistral-7B-Instruct-v0.1:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.1
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  Mistral-7B-Instruct-v0.2:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.2
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  Mistral-7B-v0.3:
    model_family: Mistral
    model_variant: 7B-v0.3
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32768
  Mistral-7B-Instruct-v0.3:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.3
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32768
  Mistral-Large-Instruct-2407:
    model_family: Mistral
    model_variant: Large-Instruct-2407
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 32768
    vllm_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
      --max-model-len: 32768
    sglang_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
      --context-length: 32768
  Mistral-Large-Instruct-2411:
    model_family: Mistral
    model_variant: Large-Instruct-2411
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 32768
    vllm_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
      --max-model-len: 32768
    sglang_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
      --context-length: 32768
  Mixtral-8x7B-Instruct-v0.1:
    model_family: Mixtral
    model_variant: 8x7B-Instruct-v0.1
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 32000
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  Mixtral-8x22B-v0.1:
    model_family: Mixtral
    model_variant: 8x22B-v0.1
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 32768
    vllm_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
    sglang_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
  Mixtral-8x22B-Instruct-v0.1:
    model_family: Mixtral
    model_variant: 8x22B-Instruct-v0.1
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 32768
    vllm_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
    sglang_args:
      --pipeline-parallel-size: 2
      --tensor-parallel-size: 4
  Phi-3-medium-128k-instruct:
    model_family: Phi-3
    model_variant: medium-128k-instruct
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32064
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  Phi-3-vision-128k-instruct:
    model_family: Phi-3-vision
    model_variant: 128k-instruct
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32064
    vllm_args:
      --tensor-parallel-size: 2
      --max-model-len: 65536
    sglang_args:
      --tensor-parallel-size: 2
      --context-length: 65536
  Llama-3.1-Nemotron-70B-Instruct-HF:
    model_family: Llama-3.1-Nemotron
    model_variant: 70B-Instruct-HF
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    vllm_args:
      --tensor-parallel-size: 4
      --max-model-len: 65536
    sglang_args:
      --tensor-parallel-size: 4
      --context-length: 65536
  Llama-3.2-1B:
    model_family: Llama-3.2
    model_variant: 1B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  Llama-3.2-1B-Instruct:
    model_family: Llama-3.2
    model_variant: 1B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  Llama-3.2-3B:
    model_family: Llama-3.2
    model_variant: 3B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  Llama-3.2-3B-Instruct:
    model_family: Llama-3.2
    model_variant: 3B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  Llama-3.2-11B-Vision-Instruct:
    model_family: Llama-3.2
    model_variant: 11B-Vision-Instruct
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 128256
    engine: sglang
    sglang_args:
      --tensor-parallel-size: 2
  Llama-3.2-90B-Vision-Instruct:
    model_family: Llama-3.2
    model_variant: 90B-Vision-Instruct
    model_type: VLM
    gpus_per_node: 4
    num_nodes: 2
    vocab_size: 128256
    engine: sglang
    sglang_args:
      --tensor-parallel-size: 8
  Qwen2.5-0.5B-Instruct:
    model_family: Qwen2.5
    model_variant: 0.5B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-1.5B-Instruct:
    model_family: Qwen2.5
    model_variant: 1.5B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-3B-Instruct:
    model_family: Qwen2.5
    model_variant: 3B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-7B-Instruct:
    model_family: Qwen2.5
    model_variant: 7B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-14B-Instruct:
    model_family: Qwen2.5
    model_variant: 14B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-32B-Instruct:
    model_family: Qwen2.5
    model_variant: 32B-Instruct
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 152064
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  Qwen2.5-72B-Instruct:
    model_family: Qwen2.5
    model_variant: 72B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 152064
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  Qwen2.5-Math-1.5B-Instruct:
    model_family: Qwen2.5-Math
    model_variant: 1.5B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-Math-7B-Instruct:
    model_family: Qwen2.5-Math
    model_variant: 7B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-Math-72B-Instruct:
    model_family: Qwen2.5-Math
    model_variant: 72B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 152064
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  Qwen2.5-Coder-3B-Instruct:
    model_family: Qwen2.5-Coder
    model_variant: 3B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-Coder-7B-Instruct:
    model_family: Qwen2.5-Coder
    model_variant: 7B-Instruct
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-Math-RM-72B:
    model_family: Qwen2.5-Math-RM
    model_variant: 72B
    model_type: Reward_Modeling
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 152064
    vllm_args:
      --tensor-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
  Qwen2.5-Math-PRM-7B:
    model_family: Qwen2.5-Math-PRM
    model_variant: 7B
    model_type: Reward_Modeling
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Qwen2.5-VL-7B-Instruct:
    model_family: Qwen2.5-VL
    model_variant: 7B-Instruct
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    vllm_args:
      --max-model-len: 32768
    sglang_args:
      --context-length: 32768
  QwQ-32B:
    model_family: QwQ
    model_variant: 32B
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 152064
    vllm_args:
      --tensor-parallel-size: 2
      --max-model-len: 32768
    sglang_args:
      --tensor-parallel-size: 2
      --context-length: 32768
  Pixtral-12B-2409:
    model_family: Pixtral
    model_variant: 12B-2409
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 131072
    vllm_args:
      --max-model-len: 8192
    sglang_args:
      --context-length: 8192
  e5-mistral-7b-instruct:
    model_family: e5
    model_variant: mistral-7b-instruct
    model_type: Text_Embedding
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 32000
  bge-base-en-v1.5:
    model_family: bge
    model_variant: base-en-v1.5
    model_type: Text_Embedding
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 30522
  all-MiniLM-L6-v2:
    model_family: all-MiniLM
    model_variant: L6-v2
    model_type: Text_Embedding
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 30522
  Llama-3.3-70B-Instruct:
    model_family: Llama-3.3
    model_variant: 70B-Instruct
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    vllm_args:
      --tensor-parallel-size: 4
      --max-model-len: 65536
    sglang_args:
      --tensor-parallel-size: 4
      --context-length: 65536
  InternVL2_5-8B:
    model_family: InternVL2_5
    model_variant: 8B
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 92553
    vllm_args:
      --trust-remote-code: true
  InternVL2_5-26B:
    model_family: InternVL2_5
    model_variant: 26B
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 92553
    vllm_args:
      --tensor-parallel-size: 2
      --trust-remote-code: true
    sglang_args:
      --tensor-parallel-size: 2
  InternVL2_5-38B:
    model_family: InternVL2_5
    model_variant: 38B
    model_type: VLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 92553
    vllm_args:
      --tensor-parallel-size: 4
      --trust-remote-code: true
    sglang_args:
      --tensor-parallel-size: 4
  aya-expanse-32b:
    model_family: aya-expanse
    model_variant: 32b
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 256000
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  DeepSeek-R1-Distill-Llama-70B:
    model_family: DeepSeek-R1
    model_variant: Distill-Llama-70B
    model_type: LLM
    gpus_per_node: 4
    num_nodes: 1
    vocab_size: 128256
    vllm_args:
      --tensor-parallel-size: 4
      --max-model-len: 65536
    sglang_args:
      --tensor-parallel-size: 4
      --context-length: 65536
  DeepSeek-R1-Distill-Llama-8B:
    model_family: DeepSeek-R1
    model_variant: Distill-Llama-8B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 128256
  DeepSeek-R1-Distill-Qwen-32B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-32B
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 152064
    vllm_args:
      --tensor-parallel-size: 2
      --max-model-len: 65536
    sglang_args:
      --tensor-parallel-size: 2
      --context-length: 65536
  DeepSeek-R1-Distill-Qwen-14B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-14B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
    vllm_args:
      --max-model-len: 65536
    sglang_args:
      --context-length: 65536
  DeepSeek-R1-Distill-Qwen-7B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-7B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  DeepSeek-R1-Distill-Qwen-1.5B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-1.5B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  Phi-3.5-vision-instruct:
    model_family: Phi-3.5-vision
    model_variant: instruct
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 32064
    vllm_args:
      --tensor-parallel-size: 2
      --max-model-len: 65536
    sglang_args:
      --tensor-parallel-size: 2
      --context-length: 65536
  glm-4v-9b:
    model_family: glm-4v
    model_variant: 9b
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 151552
  Molmo-7B-D-0924:
    model_family: Molmo
    model_variant: 7B-D-0924
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 152064
  deepseek-vl2:
    model_family: deepseek-vl2
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 129280
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  deepseek-vl2-small:
    model_family: deepseek-vl2
    model_variant: small
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 129280
  Qwen3-8B:
    model_family: Qwen3
    model_variant: 8B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 151936
  Qwen3-14B:
    model_family: Qwen3
    model_variant: 14B
    model_type: LLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 151936
  Qwen3-32B:
    model_family: Qwen3
    model_variant: 32B
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 151936
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  gpt-oss-120b:
    model_family: gpt-oss
    model_variant: 120b
    model_type: LLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 201088
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
  Llama-4-Maverick-17B-128E-Instruct:
    model_family: Llama-4
    model_variant: Maverick-17B-128E-Instruct
    model_type: VLM
    gpus_per_node: 4
    num_nodes: 4
    resource_type: h100
    cpus_per_task: 6
    mem-per-node: 60G
    vocab_size: 202048
    time: 03:00:00
    vllm_args:
      --tensor-parallel-size: 4
      --pipeline-parallel-size: 4
    sglang_args:
      --tensor-parallel-size: 4
      --pipeline-parallel-size: 4
  medgemma-4b-it:
    model_family: medgemma
    model_variant: 4b-it
    model_type: VLM
    gpus_per_node: 1
    num_nodes: 1
    vocab_size: 262208
  medgemma-27b-it:
    model_family: medgemma
    model_variant: 27b-it
    model_type: VLM
    gpus_per_node: 2
    num_nodes: 1
    vocab_size: 262208
    vllm_args:
      --tensor-parallel-size: 2
    sglang_args:
      --tensor-parallel-size: 2
