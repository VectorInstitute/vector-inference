{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vector Inference: Easy inference on Slurm clusters","text":"<p>This repository provides an easy-to-use solution to run inference servers on Slurm-managed computing clusters using vLLM. All scripts in this repository runs natively on the Vector Institute cluster environment. To adapt to other environments, update the environment variables in <code>vec_inf/client/slurm_vars.py</code>, and the model config for cached model weights in <code>vec_inf/config/models.yaml</code> accordingly.</p>"},{"location":"#installation","title":"Installation","text":"<p>If you are using the Vector cluster environment, and you don't need any customization to the inference server environment, run the following to install package:</p> <pre><code>pip install vec-inf\n</code></pre> <p>Otherwise, we recommend using the provided <code>Dockerfile</code> to set up your own environment with the package. The latest image has <code>vLLM</code> version <code>0.8.5.post1</code>.</p>"},{"location":"api/","title":"Python API Reference","text":"<p>This section documents the Python API for vector-inference.</p>"},{"location":"api/#client-interface","title":"Client Interface","text":""},{"location":"api/#vec_inf.client.api.VecInfClient","title":"vec_inf.client.api.VecInfClient","text":"<p>Client for interacting with Vector Inference programmatically.</p> <p>This class provides methods for launching models, checking their status, retrieving metrics, and shutting down models using the Vector Inference infrastructure.</p> <p>Methods:</p> Name Description <code>list_models</code> <p>List all available models</p> <code>get_model_config</code> <p>Get configuration for a specific model</p> <code>launch_model</code> <p>Launch a model on the cluster</p> <code>get_status</code> <p>Get status of a running model</p> <code>get_metrics</code> <p>Get performance metrics of a running model</p> <code>shutdown_model</code> <p>Shutdown a running model</p> <code>wait_until_ready</code> <p>Wait for a model to become ready</p> <code>cleanup_logs</code> <p>Remove logs from the log directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from vec_inf.api import VecInfClient\n&gt;&gt;&gt; client = VecInfClient()\n&gt;&gt;&gt; response = client.launch_model(\"Meta-Llama-3.1-8B-Instruct\")\n&gt;&gt;&gt; job_id = response.slurm_job_id\n&gt;&gt;&gt; status = client.get_status(job_id)\n&gt;&gt;&gt; if status.status == ModelStatus.READY:\n...     print(f\"Model is ready at {status.base_url}\")\n&gt;&gt;&gt; client.shutdown_model(job_id)\n</code></pre> Source code in <code>vec_inf/client/api.py</code> <pre><code>class VecInfClient:\n    \"\"\"Client for interacting with Vector Inference programmatically.\n\n    This class provides methods for launching models, checking their status,\n    retrieving metrics, and shutting down models using the Vector Inference\n    infrastructure.\n\n    Methods\n    -------\n    list_models()\n        List all available models\n    get_model_config(model_name)\n        Get configuration for a specific model\n    launch_model(model_name, options)\n        Launch a model on the cluster\n    get_status(slurm_job_id, log_dir)\n        Get status of a running model\n    get_metrics(slurm_job_id, log_dir)\n        Get performance metrics of a running model\n    shutdown_model(slurm_job_id)\n        Shutdown a running model\n    wait_until_ready(slurm_job_id, timeout_seconds, poll_interval_seconds, log_dir)\n        Wait for a model to become ready\n\n    cleanup_logs(log_dir, model_name, model_family, job_id, dry_run)\n        Remove logs from the log directory.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from vec_inf.api import VecInfClient\n    &gt;&gt;&gt; client = VecInfClient()\n    &gt;&gt;&gt; response = client.launch_model(\"Meta-Llama-3.1-8B-Instruct\")\n    &gt;&gt;&gt; job_id = response.slurm_job_id\n    &gt;&gt;&gt; status = client.get_status(job_id)\n    &gt;&gt;&gt; if status.status == ModelStatus.READY:\n    ...     print(f\"Model is ready at {status.base_url}\")\n    &gt;&gt;&gt; client.shutdown_model(job_id)\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the Vector Inference client.\"\"\"\n        pass\n\n    def list_models(self) -&gt; list[ModelInfo]:\n        \"\"\"List all available models.\n\n        Returns\n        -------\n        list[ModelInfo]\n            List of ModelInfo objects containing information about available models,\n            including their configurations and specifications.\n        \"\"\"\n        model_registry = ModelRegistry()\n        return model_registry.get_all_models()\n\n    def get_model_config(self, model_name: str) -&gt; ModelConfig:\n        \"\"\"Get the configuration for a specific model.\n\n        Parameters\n        ----------\n        model_name : str\n            Name of the model to get configuration for\n\n        Returns\n        -------\n        ModelConfig\n            Complete configuration for the specified model\n\n        Raises\n        ------\n        ModelNotFoundError\n            If the specified model is not found in the configuration\n        \"\"\"\n        model_registry = ModelRegistry()\n        return model_registry.get_single_model_config(model_name)\n\n    def launch_model(\n        self, model_name: str, options: Optional[LaunchOptions] = None\n    ) -&gt; LaunchResponse:\n        \"\"\"Launch a model on the cluster.\n\n        Parameters\n        ----------\n        model_name : str\n            Name of the model to launch\n        options : LaunchOptions, optional\n            Launch options to override default configuration\n\n        Returns\n        -------\n        LaunchResponse\n            Response containing launch details including:\n            - SLURM job ID\n            - Model configuration\n            - Launch status\n\n        Raises\n        ------\n        ModelConfigurationError\n            If the model configuration is invalid\n        SlurmJobError\n            If there's an error launching the SLURM job\n        \"\"\"\n        # Convert LaunchOptions to dictionary if provided\n        options_dict: dict[str, Any] = {}\n        if options:\n            options_dict = {k: v for k, v in vars(options).items() if v is not None}\n\n        # Create and use the API Launch Helper\n        model_launcher = ModelLauncher(model_name, options_dict)\n        return model_launcher.launch()\n\n    def batch_launch_models(\n        self, model_names: list[str], batch_config: Optional[str] = None\n    ) -&gt; BatchLaunchResponse:\n        \"\"\"Launch multiple models on the cluster.\n\n        Parameters\n        ----------\n        model_names : list[str]\n            List of model names to launch\n\n        Returns\n        -------\n        BatchLaunchResponse\n            Response containing launch details for each model\n\n        Raises\n        ------\n        ModelConfigurationError\n            If the model configuration is invalid\n        \"\"\"\n        model_launcher = BatchModelLauncher(model_names, batch_config)\n        return model_launcher.launch()\n\n    def get_status(self, slurm_job_id: str) -&gt; StatusResponse:\n        \"\"\"Get the status of a running model.\n\n        Parameters\n        ----------\n        slurm_job_id : str\n            The SLURM job ID to check\n\n        Returns\n        -------\n        StatusResponse\n            Status information including:\n            - Model name\n            - Server status\n            - Job state\n            - Base URL (if ready)\n            - Error information (if failed)\n        \"\"\"\n        model_status_monitor = ModelStatusMonitor(slurm_job_id)\n        return model_status_monitor.process_model_status()\n\n    def get_metrics(self, slurm_job_id: str) -&gt; MetricsResponse:\n        \"\"\"Get the performance metrics of a running model.\n\n        Parameters\n        ----------\n        slurm_job_id : str\n            The SLURM job ID to get metrics for\n\n        Returns\n        -------\n        MetricsResponse\n            Response containing:\n            - Model name\n            - Performance metrics or error message\n            - Timestamp of collection\n        \"\"\"\n        performance_metrics_collector = PerformanceMetricsCollector(slurm_job_id)\n\n        metrics: Union[dict[str, float], str]\n        if not performance_metrics_collector.metrics_url.startswith(\"http\"):\n            metrics = performance_metrics_collector.metrics_url\n        else:\n            metrics = performance_metrics_collector.fetch_metrics()\n\n        return MetricsResponse(\n            model_name=performance_metrics_collector.status_info.model_name,\n            metrics=metrics,\n            timestamp=time.time(),\n        )\n\n    def shutdown_model(self, slurm_job_id: str) -&gt; bool:\n        \"\"\"Shutdown a running model.\n\n        Parameters\n        ----------\n        slurm_job_id : str\n            The SLURM job ID to shut down\n\n        Returns\n        -------\n        bool\n            True if the model was successfully shutdown\n\n        Raises\n        ------\n        SlurmJobError\n            If there was an error shutting down the model\n        \"\"\"\n        shutdown_cmd = f\"scancel {slurm_job_id}\"\n        _, stderr = run_bash_command(shutdown_cmd)\n        if stderr:\n            raise SlurmJobError(f\"Failed to shutdown model: {stderr}\")\n        return True\n\n    def wait_until_ready(\n        self,\n        slurm_job_id: str,\n        timeout_seconds: int = 1800,\n        poll_interval_seconds: int = 10,\n    ) -&gt; StatusResponse:\n        \"\"\"Wait until a model is ready or fails.\n\n        Parameters\n        ----------\n        slurm_job_id : str\n            The SLURM job ID to wait for\n        timeout_seconds : int, optional\n            Maximum time to wait in seconds, by default 1800 (30 mins)\n        poll_interval_seconds : int, optional\n            How often to check status in seconds, by default 10\n\n        Returns\n        -------\n        StatusResponse\n            Status information when the model becomes ready\n\n        Raises\n        ------\n        SlurmJobError\n            If the specified job is not found or there's an error with the job\n        ServerError\n            If the server fails to start within the timeout period\n        APIError\n            If there was an error checking the status\n\n        Notes\n        -----\n        The timeout is reset if the model is still in PENDING state after the\n        initial timeout period. This allows for longer queue times in the SLURM\n        scheduler.\n        \"\"\"\n        start_time = time.time()\n\n        while True:\n            status_info = self.get_status(slurm_job_id)\n\n            if status_info.server_status == ModelStatus.READY:\n                return status_info\n\n            if status_info.server_status == ModelStatus.FAILED:\n                error_message = status_info.failed_reason or \"Unknown error\"\n                raise ServerError(f\"Model failed to start: {error_message}\")\n\n            if status_info.server_status == ModelStatus.SHUTDOWN:\n                raise ServerError(\"Model was shutdown before it became ready\")\n\n            # Check timeout\n            if time.time() - start_time &gt; timeout_seconds:\n                if status_info.server_status == ModelStatus.PENDING:\n                    warnings.warn(\n                        f\"Model is still pending after {timeout_seconds} seconds, resetting timer...\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n                    start_time = time.time()\n                raise ServerError(\n                    f\"Timed out waiting for model to become ready after {timeout_seconds} seconds\"\n                )\n\n            # Wait before checking again\n            time.sleep(poll_interval_seconds)\n\n    def cleanup_logs(\n        self,\n        log_dir: Optional[Union[str, Path]] = None,\n        model_family: Optional[str] = None,\n        model_name: Optional[str] = None,\n        job_id: Optional[int] = None,\n        before_job_id: Optional[int] = None,\n        dry_run: bool = False,\n    ) -&gt; list[Path]:\n        \"\"\"Remove logs from the log directory.\n\n        Parameters\n        ----------\n        log_dir : str or Path, optional\n            Root directory containing log files. Defaults to ~/.vec-inf-logs.\n        model_family : str, optional\n            Only delete logs for this model family.\n        model_name : str, optional\n            Only delete logs for this model name.\n        job_id : int, optional\n            If provided, only match directories with this exact SLURM job ID.\n        before_job_id : int, optional\n            If provided, only delete logs with job ID less than this value.\n        dry_run : bool\n            If True, return matching files without deleting them.\n\n        Returns\n        -------\n        list[Path]\n            List of deleted (or matched if dry_run) log file paths.\n        \"\"\"\n        log_root = Path(log_dir) if log_dir else Path.home() / \".vec-inf-logs\"\n        matched = find_matching_dirs(\n            log_dir=log_root,\n            model_family=model_family,\n            model_name=model_name,\n            job_id=job_id,\n            before_job_id=before_job_id,\n        )\n\n        if dry_run:\n            return matched\n\n        for path in matched:\n            shutil.rmtree(path)\n\n        return matched\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the Vector Inference client.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the Vector Inference client.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.list_models","title":"list_models","text":"<pre><code>list_models()\n</code></pre> <p>List all available models.</p> <p>Returns:</p> Type Description <code>list[ModelInfo]</code> <p>List of ModelInfo objects containing information about available models, including their configurations and specifications.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def list_models(self) -&gt; list[ModelInfo]:\n    \"\"\"List all available models.\n\n    Returns\n    -------\n    list[ModelInfo]\n        List of ModelInfo objects containing information about available models,\n        including their configurations and specifications.\n    \"\"\"\n    model_registry = ModelRegistry()\n    return model_registry.get_all_models()\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.get_model_config","title":"get_model_config","text":"<pre><code>get_model_config(model_name)\n</code></pre> <p>Get the configuration for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to get configuration for</p> required <p>Returns:</p> Type Description <code>ModelConfig</code> <p>Complete configuration for the specified model</p> <p>Raises:</p> Type Description <code>ModelNotFoundError</code> <p>If the specified model is not found in the configuration</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def get_model_config(self, model_name: str) -&gt; ModelConfig:\n    \"\"\"Get the configuration for a specific model.\n\n    Parameters\n    ----------\n    model_name : str\n        Name of the model to get configuration for\n\n    Returns\n    -------\n    ModelConfig\n        Complete configuration for the specified model\n\n    Raises\n    ------\n    ModelNotFoundError\n        If the specified model is not found in the configuration\n    \"\"\"\n    model_registry = ModelRegistry()\n    return model_registry.get_single_model_config(model_name)\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.launch_model","title":"launch_model","text":"<pre><code>launch_model(model_name, options=None)\n</code></pre> <p>Launch a model on the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to launch</p> required <code>options</code> <code>LaunchOptions</code> <p>Launch options to override default configuration</p> <code>None</code> <p>Returns:</p> Type Description <code>LaunchResponse</code> <p>Response containing launch details including: - SLURM job ID - Model configuration - Launch status</p> <p>Raises:</p> Type Description <code>ModelConfigurationError</code> <p>If the model configuration is invalid</p> <code>SlurmJobError</code> <p>If there's an error launching the SLURM job</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def launch_model(\n    self, model_name: str, options: Optional[LaunchOptions] = None\n) -&gt; LaunchResponse:\n    \"\"\"Launch a model on the cluster.\n\n    Parameters\n    ----------\n    model_name : str\n        Name of the model to launch\n    options : LaunchOptions, optional\n        Launch options to override default configuration\n\n    Returns\n    -------\n    LaunchResponse\n        Response containing launch details including:\n        - SLURM job ID\n        - Model configuration\n        - Launch status\n\n    Raises\n    ------\n    ModelConfigurationError\n        If the model configuration is invalid\n    SlurmJobError\n        If there's an error launching the SLURM job\n    \"\"\"\n    # Convert LaunchOptions to dictionary if provided\n    options_dict: dict[str, Any] = {}\n    if options:\n        options_dict = {k: v for k, v in vars(options).items() if v is not None}\n\n    # Create and use the API Launch Helper\n    model_launcher = ModelLauncher(model_name, options_dict)\n    return model_launcher.launch()\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.batch_launch_models","title":"batch_launch_models","text":"<pre><code>batch_launch_models(model_names, batch_config=None)\n</code></pre> <p>Launch multiple models on the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>model_names</code> <code>list[str]</code> <p>List of model names to launch</p> required <p>Returns:</p> Type Description <code>BatchLaunchResponse</code> <p>Response containing launch details for each model</p> <p>Raises:</p> Type Description <code>ModelConfigurationError</code> <p>If the model configuration is invalid</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def batch_launch_models(\n    self, model_names: list[str], batch_config: Optional[str] = None\n) -&gt; BatchLaunchResponse:\n    \"\"\"Launch multiple models on the cluster.\n\n    Parameters\n    ----------\n    model_names : list[str]\n        List of model names to launch\n\n    Returns\n    -------\n    BatchLaunchResponse\n        Response containing launch details for each model\n\n    Raises\n    ------\n    ModelConfigurationError\n        If the model configuration is invalid\n    \"\"\"\n    model_launcher = BatchModelLauncher(model_names, batch_config)\n    return model_launcher.launch()\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.get_status","title":"get_status","text":"<pre><code>get_status(slurm_job_id)\n</code></pre> <p>Get the status of a running model.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>str</code> <p>The SLURM job ID to check</p> required <p>Returns:</p> Type Description <code>StatusResponse</code> <p>Status information including: - Model name - Server status - Job state - Base URL (if ready) - Error information (if failed)</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def get_status(self, slurm_job_id: str) -&gt; StatusResponse:\n    \"\"\"Get the status of a running model.\n\n    Parameters\n    ----------\n    slurm_job_id : str\n        The SLURM job ID to check\n\n    Returns\n    -------\n    StatusResponse\n        Status information including:\n        - Model name\n        - Server status\n        - Job state\n        - Base URL (if ready)\n        - Error information (if failed)\n    \"\"\"\n    model_status_monitor = ModelStatusMonitor(slurm_job_id)\n    return model_status_monitor.process_model_status()\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics(slurm_job_id)\n</code></pre> <p>Get the performance metrics of a running model.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>str</code> <p>The SLURM job ID to get metrics for</p> required <p>Returns:</p> Type Description <code>MetricsResponse</code> <p>Response containing: - Model name - Performance metrics or error message - Timestamp of collection</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def get_metrics(self, slurm_job_id: str) -&gt; MetricsResponse:\n    \"\"\"Get the performance metrics of a running model.\n\n    Parameters\n    ----------\n    slurm_job_id : str\n        The SLURM job ID to get metrics for\n\n    Returns\n    -------\n    MetricsResponse\n        Response containing:\n        - Model name\n        - Performance metrics or error message\n        - Timestamp of collection\n    \"\"\"\n    performance_metrics_collector = PerformanceMetricsCollector(slurm_job_id)\n\n    metrics: Union[dict[str, float], str]\n    if not performance_metrics_collector.metrics_url.startswith(\"http\"):\n        metrics = performance_metrics_collector.metrics_url\n    else:\n        metrics = performance_metrics_collector.fetch_metrics()\n\n    return MetricsResponse(\n        model_name=performance_metrics_collector.status_info.model_name,\n        metrics=metrics,\n        timestamp=time.time(),\n    )\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.shutdown_model","title":"shutdown_model","text":"<pre><code>shutdown_model(slurm_job_id)\n</code></pre> <p>Shutdown a running model.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>str</code> <p>The SLURM job ID to shut down</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the model was successfully shutdown</p> <p>Raises:</p> Type Description <code>SlurmJobError</code> <p>If there was an error shutting down the model</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def shutdown_model(self, slurm_job_id: str) -&gt; bool:\n    \"\"\"Shutdown a running model.\n\n    Parameters\n    ----------\n    slurm_job_id : str\n        The SLURM job ID to shut down\n\n    Returns\n    -------\n    bool\n        True if the model was successfully shutdown\n\n    Raises\n    ------\n    SlurmJobError\n        If there was an error shutting down the model\n    \"\"\"\n    shutdown_cmd = f\"scancel {slurm_job_id}\"\n    _, stderr = run_bash_command(shutdown_cmd)\n    if stderr:\n        raise SlurmJobError(f\"Failed to shutdown model: {stderr}\")\n    return True\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.wait_until_ready","title":"wait_until_ready","text":"<pre><code>wait_until_ready(\n    slurm_job_id,\n    timeout_seconds=1800,\n    poll_interval_seconds=10,\n)\n</code></pre> <p>Wait until a model is ready or fails.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>str</code> <p>The SLURM job ID to wait for</p> required <code>timeout_seconds</code> <code>int</code> <p>Maximum time to wait in seconds, by default 1800 (30 mins)</p> <code>1800</code> <code>poll_interval_seconds</code> <code>int</code> <p>How often to check status in seconds, by default 10</p> <code>10</code> <p>Returns:</p> Type Description <code>StatusResponse</code> <p>Status information when the model becomes ready</p> <p>Raises:</p> Type Description <code>SlurmJobError</code> <p>If the specified job is not found or there's an error with the job</p> <code>ServerError</code> <p>If the server fails to start within the timeout period</p> <code>APIError</code> <p>If there was an error checking the status</p> Notes <p>The timeout is reset if the model is still in PENDING state after the initial timeout period. This allows for longer queue times in the SLURM scheduler.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def wait_until_ready(\n    self,\n    slurm_job_id: str,\n    timeout_seconds: int = 1800,\n    poll_interval_seconds: int = 10,\n) -&gt; StatusResponse:\n    \"\"\"Wait until a model is ready or fails.\n\n    Parameters\n    ----------\n    slurm_job_id : str\n        The SLURM job ID to wait for\n    timeout_seconds : int, optional\n        Maximum time to wait in seconds, by default 1800 (30 mins)\n    poll_interval_seconds : int, optional\n        How often to check status in seconds, by default 10\n\n    Returns\n    -------\n    StatusResponse\n        Status information when the model becomes ready\n\n    Raises\n    ------\n    SlurmJobError\n        If the specified job is not found or there's an error with the job\n    ServerError\n        If the server fails to start within the timeout period\n    APIError\n        If there was an error checking the status\n\n    Notes\n    -----\n    The timeout is reset if the model is still in PENDING state after the\n    initial timeout period. This allows for longer queue times in the SLURM\n    scheduler.\n    \"\"\"\n    start_time = time.time()\n\n    while True:\n        status_info = self.get_status(slurm_job_id)\n\n        if status_info.server_status == ModelStatus.READY:\n            return status_info\n\n        if status_info.server_status == ModelStatus.FAILED:\n            error_message = status_info.failed_reason or \"Unknown error\"\n            raise ServerError(f\"Model failed to start: {error_message}\")\n\n        if status_info.server_status == ModelStatus.SHUTDOWN:\n            raise ServerError(\"Model was shutdown before it became ready\")\n\n        # Check timeout\n        if time.time() - start_time &gt; timeout_seconds:\n            if status_info.server_status == ModelStatus.PENDING:\n                warnings.warn(\n                    f\"Model is still pending after {timeout_seconds} seconds, resetting timer...\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                start_time = time.time()\n            raise ServerError(\n                f\"Timed out waiting for model to become ready after {timeout_seconds} seconds\"\n            )\n\n        # Wait before checking again\n        time.sleep(poll_interval_seconds)\n</code></pre>"},{"location":"api/#vec_inf.client.api.VecInfClient.cleanup_logs","title":"cleanup_logs","text":"<pre><code>cleanup_logs(\n    log_dir=None,\n    model_family=None,\n    model_name=None,\n    job_id=None,\n    before_job_id=None,\n    dry_run=False,\n)\n</code></pre> <p>Remove logs from the log directory.</p> <p>Parameters:</p> Name Type Description Default <code>log_dir</code> <code>str or Path</code> <p>Root directory containing log files. Defaults to ~/.vec-inf-logs.</p> <code>None</code> <code>model_family</code> <code>str</code> <p>Only delete logs for this model family.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Only delete logs for this model name.</p> <code>None</code> <code>job_id</code> <code>int</code> <p>If provided, only match directories with this exact SLURM job ID.</p> <code>None</code> <code>before_job_id</code> <code>int</code> <p>If provided, only delete logs with job ID less than this value.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, return matching files without deleting them.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of deleted (or matched if dry_run) log file paths.</p> Source code in <code>vec_inf/client/api.py</code> <pre><code>def cleanup_logs(\n    self,\n    log_dir: Optional[Union[str, Path]] = None,\n    model_family: Optional[str] = None,\n    model_name: Optional[str] = None,\n    job_id: Optional[int] = None,\n    before_job_id: Optional[int] = None,\n    dry_run: bool = False,\n) -&gt; list[Path]:\n    \"\"\"Remove logs from the log directory.\n\n    Parameters\n    ----------\n    log_dir : str or Path, optional\n        Root directory containing log files. Defaults to ~/.vec-inf-logs.\n    model_family : str, optional\n        Only delete logs for this model family.\n    model_name : str, optional\n        Only delete logs for this model name.\n    job_id : int, optional\n        If provided, only match directories with this exact SLURM job ID.\n    before_job_id : int, optional\n        If provided, only delete logs with job ID less than this value.\n    dry_run : bool\n        If True, return matching files without deleting them.\n\n    Returns\n    -------\n    list[Path]\n        List of deleted (or matched if dry_run) log file paths.\n    \"\"\"\n    log_root = Path(log_dir) if log_dir else Path.home() / \".vec-inf-logs\"\n    matched = find_matching_dirs(\n        log_dir=log_root,\n        model_family=model_family,\n        model_name=model_name,\n        job_id=job_id,\n        before_job_id=before_job_id,\n    )\n\n    if dry_run:\n        return matched\n\n    for path in matched:\n        shutil.rmtree(path)\n\n    return matched\n</code></pre>"},{"location":"api/#data-models","title":"Data Models","text":""},{"location":"api/#vec_inf.client.models","title":"vec_inf.client.models","text":"<p>Data models for Vector Inference API.</p> <p>This module contains the data model classes used by the Vector Inference API for both request parameters and response objects.</p> <p>Classes:</p> Name Description <code>ModelStatus : Enum</code> <p>Status states of a model</p> <code>ModelType : Enum</code> <p>Types of supported models</p> <code>LaunchResponse : dataclass</code> <p>Response from model launch operation</p> <code>StatusResponse : dataclass</code> <p>Response from model status check</p> <code>MetricsResponse : dataclass</code> <p>Response from metrics collection</p> <code>LaunchOptions : dataclass</code> <p>Options for model launch</p> <code>LaunchOptionsDict : TypedDict</code> <p>Dictionary representation of launch options</p> <code>ModelInfo : datacitten</code> <p>Information about available models</p>"},{"location":"api/#vec_inf.client.models.ModelStatus","title":"ModelStatus","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum representing the possible status states of a model.</p> <p>Attributes:</p> Name Type Description <code>PENDING</code> <code>str</code> <p>Model is waiting for Slurm to allocate resources</p> <code>LAUNCHING</code> <code>str</code> <p>Model is in the process of starting</p> <code>READY</code> <code>str</code> <p>Model is running and ready to serve requests</p> <code>FAILED</code> <code>str</code> <p>Model failed to start or encountered an error</p> <code>SHUTDOWN</code> <code>str</code> <p>Model was intentionally stopped</p> <code>UNAVAILABLE</code> <code>str</code> <p>Model status cannot be determined</p> Source code in <code>vec_inf/client/models.py</code> <pre><code>class ModelStatus(str, Enum):\n    \"\"\"Enum representing the possible status states of a model.\n\n    Attributes\n    ----------\n    PENDING : str\n        Model is waiting for Slurm to allocate resources\n    LAUNCHING : str\n        Model is in the process of starting\n    READY : str\n        Model is running and ready to serve requests\n    FAILED : str\n        Model failed to start or encountered an error\n    SHUTDOWN : str\n        Model was intentionally stopped\n    UNAVAILABLE : str\n        Model status cannot be determined\n    \"\"\"\n\n    PENDING = \"PENDING\"\n    LAUNCHING = \"LAUNCHING\"\n    READY = \"READY\"\n    FAILED = \"FAILED\"\n    SHUTDOWN = \"SHUTDOWN\"\n    UNAVAILABLE = \"UNAVAILABLE\"\n</code></pre>"},{"location":"api/#vec_inf.client.models.ModelType","title":"ModelType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum representing the possible model types.</p> <p>Attributes:</p> Name Type Description <code>LLM</code> <code>str</code> <p>Large Language Model</p> <code>VLM</code> <code>str</code> <p>Vision Language Model</p> <code>TEXT_EMBEDDING</code> <code>str</code> <p>Text Embedding Model</p> <code>REWARD_MODELING</code> <code>str</code> <p>Reward Modeling Model</p> Source code in <code>vec_inf/client/models.py</code> <pre><code>class ModelType(str, Enum):\n    \"\"\"Enum representing the possible model types.\n\n    Attributes\n    ----------\n    LLM : str\n        Large Language Model\n    VLM : str\n        Vision Language Model\n    TEXT_EMBEDDING : str\n        Text Embedding Model\n    REWARD_MODELING : str\n        Reward Modeling Model\n    \"\"\"\n\n    LLM = \"LLM\"\n    VLM = \"VLM\"\n    TEXT_EMBEDDING = \"Text_Embedding\"\n    REWARD_MODELING = \"Reward_Modeling\"\n</code></pre>"},{"location":"api/#vec_inf.client.models.LaunchResponse","title":"LaunchResponse  <code>dataclass</code>","text":"<p>Response from launching a model.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>str</code> <p>ID of the launched SLURM job</p> required <code>model_name</code> <code>str</code> <p>Name of the launched model</p> required <code>config</code> <code>dict[str, Any]</code> <p>Configuration used for the launch</p> required <code>raw_output</code> <code>str</code> <p>Raw output from the launch command (hidden from repr)</p> required Source code in <code>vec_inf/client/models.py</code> <pre><code>@dataclass\nclass LaunchResponse:\n    \"\"\"Response from launching a model.\n\n    Parameters\n    ----------\n    slurm_job_id : str\n        ID of the launched SLURM job\n    model_name : str\n        Name of the launched model\n    config : dict[str, Any]\n        Configuration used for the launch\n    raw_output : str\n        Raw output from the launch command (hidden from repr)\n    \"\"\"\n\n    slurm_job_id: str\n    model_name: str\n    config: dict[str, Any]\n    raw_output: str = field(repr=False)\n</code></pre>"},{"location":"api/#vec_inf.client.models.BatchLaunchResponse","title":"BatchLaunchResponse  <code>dataclass</code>","text":"<p>Response from launching multiple models in batch mode.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_job_id</code> <code>str</code> <p>ID of the launched SLURM job</p> required <code>slurm_job_name</code> <code>str</code> <p>Name of the launched SLURM job</p> required <code>model_names</code> <code>list[str]</code> <p>Names of the launched models</p> required <code>config</code> <code>dict[str, Any]</code> <p>Configuration used for the launch</p> required <code>raw_output</code> <code>str</code> <p>Raw output from the launch command (hidden from repr)</p> required Source code in <code>vec_inf/client/models.py</code> <pre><code>@dataclass\nclass BatchLaunchResponse:\n    \"\"\"Response from launching multiple models in batch mode.\n\n    Parameters\n    ----------\n    slurm_job_id : str\n        ID of the launched SLURM job\n    slurm_job_name : str\n        Name of the launched SLURM job\n    model_names : list[str]\n        Names of the launched models\n    config : dict[str, Any]\n        Configuration used for the launch\n    raw_output : str\n        Raw output from the launch command (hidden from repr)\n    \"\"\"\n\n    slurm_job_id: str\n    slurm_job_name: str\n    model_names: list[str]\n    config: dict[str, Any]\n    raw_output: str = field(repr=False)\n</code></pre>"},{"location":"api/#vec_inf.client.models.StatusResponse","title":"StatusResponse  <code>dataclass</code>","text":"<p>Response from checking a model's status.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <code>log_dir</code> <code>str</code> <p>Path to the SLURM log directory</p> required <code>server_status</code> <code>ModelStatus</code> <p>Current status of the server</p> required <code>job_state</code> <code>Union[str, ModelStatus]</code> <p>Current state of the SLURM job</p> required <code>raw_output</code> <code>str</code> <p>Raw output from status check (hidden from repr)</p> required <code>base_url</code> <code>str</code> <p>Base URL of the model server if ready</p> <code>None</code> <code>pending_reason</code> <code>str</code> <p>Reason for pending state if applicable</p> <code>None</code> <code>failed_reason</code> <code>str</code> <p>Reason for failure if applicable</p> <code>None</code> Source code in <code>vec_inf/client/models.py</code> <pre><code>@dataclass\nclass StatusResponse:\n    \"\"\"Response from checking a model's status.\n\n    Parameters\n    ----------\n    model_name : str\n        Name of the model\n    log_dir : str\n        Path to the SLURM log directory\n    server_status : ModelStatus\n        Current status of the server\n    job_state : Union[str, ModelStatus]\n        Current state of the SLURM job\n    raw_output : str\n        Raw output from status check (hidden from repr)\n    base_url : str, optional\n        Base URL of the model server if ready\n    pending_reason : str, optional\n        Reason for pending state if applicable\n    failed_reason : str, optional\n        Reason for failure if applicable\n    \"\"\"\n\n    model_name: str\n    log_dir: str\n    server_status: ModelStatus\n    job_state: Union[str, ModelStatus]\n    raw_output: str = field(repr=False)\n    base_url: Optional[str] = None\n    pending_reason: Optional[str] = None\n    failed_reason: Optional[str] = None\n</code></pre>"},{"location":"api/#vec_inf.client.models.MetricsResponse","title":"MetricsResponse  <code>dataclass</code>","text":"<p>Response from retrieving model metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <code>metrics</code> <code>Union[dict[str, float], str]</code> <p>Either a dictionary of metrics or an error message</p> required <code>timestamp</code> <code>float</code> <p>Unix timestamp of when metrics were collected</p> required Source code in <code>vec_inf/client/models.py</code> <pre><code>@dataclass\nclass MetricsResponse:\n    \"\"\"Response from retrieving model metrics.\n\n    Parameters\n    ----------\n    model_name : str\n        Name of the model\n    metrics : Union[dict[str, float], str]\n        Either a dictionary of metrics or an error message\n    timestamp : float\n        Unix timestamp of when metrics were collected\n    \"\"\"\n\n    model_name: str\n    metrics: Union[dict[str, float], str]\n    timestamp: float\n</code></pre>"},{"location":"api/#vec_inf.client.models.LaunchOptions","title":"LaunchOptions  <code>dataclass</code>","text":"<p>Options for launching a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_family</code> <code>str</code> <p>Family/architecture of the model</p> <code>None</code> <code>model_variant</code> <code>str</code> <p>Specific variant/version of the model</p> <code>None</code> <code>partition</code> <code>str</code> <p>SLURM partition to use</p> <code>None</code> <code>num_nodes</code> <code>int</code> <p>Number of nodes to allocate</p> <code>None</code> <code>gpus_per_node</code> <code>int</code> <p>Number of GPUs per node</p> <code>None</code> <code>account</code> <code>str</code> <p>Account name for job scheduling</p> <code>None</code> <code>qos</code> <code>str</code> <p>Quality of Service level</p> <code>None</code> <code>time</code> <code>str</code> <p>Time limit for the job</p> <code>None</code> <code>exclude</code> <code>str</code> <p>Exclude certain nodes from the resources granted to the job</p> <code>None</code> <code>node_list</code> <code>str</code> <p>Request a specific list of nodes for deployment</p> required <code>bind</code> <code>str</code> <p>Additional binds for the singularity container</p> <code>None</code> <code>vocab_size</code> <code>int</code> <p>Size of model vocabulary</p> <code>None</code> <code>data_type</code> <code>str</code> <p>Data type for model weights</p> <code>None</code> <code>venv</code> <code>str</code> <p>Virtual environment to use</p> <code>None</code> <code>log_dir</code> <code>str</code> <p>Directory for logs</p> <code>None</code> <code>model_weights_parent_dir</code> <code>str</code> <p>Parent directory containing model weights</p> <code>None</code> <code>vllm_args</code> <code>str</code> <p>Additional arguments for vLLM</p> <code>None</code> Source code in <code>vec_inf/client/models.py</code> <pre><code>@dataclass\nclass LaunchOptions:\n    \"\"\"Options for launching a model.\n\n    Parameters\n    ----------\n    model_family : str, optional\n        Family/architecture of the model\n    model_variant : str, optional\n        Specific variant/version of the model\n    partition : str, optional\n        SLURM partition to use\n    num_nodes : int, optional\n        Number of nodes to allocate\n    gpus_per_node : int, optional\n        Number of GPUs per node\n    account : str, optional\n        Account name for job scheduling\n    qos : str, optional\n        Quality of Service level\n    time : str, optional\n        Time limit for the job\n    exclude : str, optional\n        Exclude certain nodes from the resources granted to the job\n    node_list : str, optional\n        Request a specific list of nodes for deployment\n    bind : str, optional\n        Additional binds for the singularity container\n    vocab_size : int, optional\n        Size of model vocabulary\n    data_type : str, optional\n        Data type for model weights\n    venv : str, optional\n        Virtual environment to use\n    log_dir : str, optional\n        Directory for logs\n    model_weights_parent_dir : str, optional\n        Parent directory containing model weights\n    vllm_args : str, optional\n        Additional arguments for vLLM\n    \"\"\"\n\n    model_family: Optional[str] = None\n    model_variant: Optional[str] = None\n    partition: Optional[str] = None\n    num_nodes: Optional[int] = None\n    gpus_per_node: Optional[int] = None\n    account: Optional[str] = None\n    qos: Optional[str] = None\n    exclude: Optional[str] = None\n    nodelist: Optional[str] = None\n    bind: Optional[str] = None\n    time: Optional[str] = None\n    vocab_size: Optional[int] = None\n    data_type: Optional[str] = None\n    venv: Optional[str] = None\n    log_dir: Optional[str] = None\n    model_weights_parent_dir: Optional[str] = None\n    vllm_args: Optional[str] = None\n</code></pre>"},{"location":"api/#vec_inf.client.models.ModelInfo","title":"ModelInfo  <code>dataclass</code>","text":"<p>Information about an available model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model</p> required <code>family</code> <code>str</code> <p>Family/architecture of the model</p> required <code>variant</code> <code>str</code> <p>Specific variant/version of the model</p> required <code>model_type</code> <code>ModelType</code> <p>Type of the model</p> required <code>config</code> <code>dict[str, Any]</code> <p>Additional configuration parameters</p> required Source code in <code>vec_inf/client/models.py</code> <pre><code>@dataclass\nclass ModelInfo:\n    \"\"\"Information about an available model.\n\n    Parameters\n    ----------\n    name : str\n        Name of the model\n    family : str\n        Family/architecture of the model\n    variant : str, optional\n        Specific variant/version of the model\n    model_type : ModelType\n        Type of the model\n    config : dict[str, Any]\n        Additional configuration parameters\n    \"\"\"\n\n    name: str\n    family: str\n    variant: Optional[str]\n    model_type: ModelType\n    config: dict[str, Any]\n</code></pre>"},{"location":"contributing/","title":"Contributing to Vector Inference","text":"<p>Thank you for your interest in contributing to Vector Inference! This guide will help you get started with development, testing, and documentation contributions.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or newer</li> <li>uv for dependency management</li> </ul>"},{"location":"contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/VectorInstitute/vector-inference.git\ncd vector-inference\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>uv sync --all-extras --group dev\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol> <p>Using Virtual Environments</p> <p>If you prefer using virtual environments, you can use <code>uv venv</code> to create one: <pre><code>uv venv\nsource .venv/bin/activate\n</code></pre></p>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#code-style-and-linting","title":"Code Style and Linting","text":"<p>We use several tools to ensure code quality:</p> <ul> <li>ruff for linting and formatting</li> <li>mypy for type checking</li> </ul> <p>You can run these tools with:</p> <pre><code># Linting\nuv run ruff check .\n\n# Type checking\nuv run mypy\n\n# Format code\nuv run ruff format .\n</code></pre> <p>Pre-commit Hooks</p> <p>The pre-commit hooks will automatically run these checks before each commit. If the hooks fail, you will need to fix the issues before you can commit.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>All new features and bug fixes should include tests. We use pytest for testing:</p> <pre><code># Run all tests\nuv run pytest\n\n# Run tests with coverage\nuv run pytest --cov=vec_inf\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#documentation-setup","title":"Documentation Setup","text":"<p>Install the documentation dependencies:</p> <pre><code>uv sync --group docs\n</code></pre>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<p>Build and serve the documentation locally:</p> <pre><code># Standard build\nmkdocs build\n\n# Serve locally with hot-reload\nmkdocs serve\n</code></pre>"},{"location":"contributing/#versioned-documentation","title":"Versioned Documentation","text":"<p>Vector Inference uses mike to manage versioned documentation. This allows users to access documentation for specific versions of the library.</p>"},{"location":"contributing/#available-versions","title":"Available Versions","text":"<p>The documentation is available in multiple versions:</p> <ul> <li><code>latest</code> - Always points to the most recent stable release</li> <li>Version-specific documentation (e.g., <code>0.5.0</code>, <code>0.4.0</code>)</li> </ul>"},{"location":"contributing/#versioning-strategy","title":"Versioning Strategy","text":"<p>Our versioning strategy follows these rules:</p> <ol> <li>Each release gets its own version number matching the package version (e.g., <code>0.5.0</code>)</li> <li>The <code>latest</code> alias always points to the most recent stable release</li> <li>Documentation is automatically deployed when changes are pushed to the main branch</li> </ol>"},{"location":"contributing/#working-with-mike-locally","title":"Working with Mike Locally","text":"<p>To preview or work with versioned documentation:</p> <pre><code># Build and deploy a specific version to your local gh-pages branch\nmike deploy 0.5.0\n\n# Add an alias for the latest version\nmike deploy 0.5.0 latest\n\n# Set the default version to redirect to\nmike set-default latest\n\n# View the deployed versions\nmike list\n\n# Serve the versioned documentation locally\nmike serve\n</code></pre>"},{"location":"contributing/#automatic-documentation-deployment","title":"Automatic Documentation Deployment","text":"<p>Documentation is automatically deployed through GitHub Actions:</p> <ul> <li>On pushes to <code>main</code>, documentation is deployed with the version from <code>pyproject.toml</code> and the <code>latest</code> alias</li> <li>Through manual trigger in the GitHub Actions workflow, where you can specify the version to deploy</li> </ul> <p>When to Update Documentation</p> <ul> <li>When adding new features</li> <li>When changing existing APIs</li> <li>When fixing bugs that affect user experience</li> <li>When improving explanations or examples</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository and create your branch from <code>main</code></li> <li>Make your changes and add appropriate tests</li> <li>Ensure tests pass and code meets style guidelines</li> <li>Write clear documentation for your changes</li> <li>Submit a pull request with a clear description of the changes</li> </ol> <p>Checklist Before Submitting PR</p> <ul> <li>[ ] All tests pass</li> <li>[ ] Code is formatted with ruff</li> <li>[ ] Type annotations are correct</li> <li>[ ] Documentation is updated</li> <li>[ ] Commit messages are clear and descriptive</li> </ul>"},{"location":"contributing/#release-process","title":"Release Process","text":"<ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update changelogs and documentation as needed</li> <li>Create a new tag and release on GitHub</li> <li>Documentation for the new version will be automatically deployed</li> </ol>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to Vector Inference, you agree that your contributions will be licensed under the project's MIT License.</p>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#cli-usage","title":"CLI Usage","text":""},{"location":"user_guide/#launch-command","title":"<code>launch</code> command","text":"<p>The <code>launch</code> command allows users to launch a OpenAI-compatible model inference server as a slurm job. If the job successfully launches, a URL endpoint is exposed for the user to send requests for inference.</p> <p>We will use the Llama 3.1 model as example, to launch an OpenAI compatible inference server for Meta-Llama-3.1-8B-Instruct, run:</p> <p><pre><code>vec-inf launch Meta-Llama-3.1-8B-Instruct\n</code></pre> You should see an output like the following:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Job Config              \u2503 Value                                     \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Slurm Job ID            \u2502 16060964                                  \u2502\n\u2502 Job Name                \u2502 Meta-Llama-3.1-8B-Instruct                \u2502\n\u2502 Model Type              \u2502 LLM                                       \u2502\n\u2502 Vocabulary Size         \u2502 128256                                    \u2502\n\u2502 Partition               \u2502 a40                                       \u2502\n\u2502 QoS                     \u2502 m2                                        \u2502\n\u2502 Time Limit              \u2502 08:00:00                                  \u2502\n\u2502 Num Nodes               \u2502 1                                         \u2502\n\u2502 GPUs/Node               \u2502 1                                         \u2502\n\u2502 CPUs/Task               \u2502 16                                        \u2502\n\u2502 Memory/Node             \u2502 64G                                       \u2502\n\u2502 Model Weights Directory \u2502 /model-weights/Meta-Llama-3.1-8B-Instruct \u2502\n\u2502 Log Directory           \u2502 /h/vi_user/.vec-inf-logs/Meta-Llama-3.1   \u2502\n\u2502 vLLM Arguments:         \u2502                                           \u2502\n\u2502   --max-model-len:      \u2502 131072                                    \u2502\n\u2502   --max-num-seqs:       \u2502 256                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user_guide/#overrides","title":"Overrides","text":"<p>Models that are already supported by <code>vec-inf</code> would be launched using the cached configuration or default configuration. You can override these values by providing additional parameters. Use <code>vec-inf launch --help</code> to see the full list of parameters that can be overriden. For example, if <code>qos</code> is to be overriden:</p> <pre><code>vec-inf launch Meta-Llama-3.1-8B-Instruct --qos &lt;new_qos&gt;\n</code></pre> <p>To overwrite default vLLM engine arguments, you can specify the engine arguments in a comma separated string:</p> <pre><code>vec-inf launch Meta-Llama-3.1-8B-Instruct --vllm-args '--max-model-len=65536,--compilation-config=3'\n</code></pre> <p>For the full list of vLLM engine arguments, you can find them here, make sure you select the correct vLLM version.</p>"},{"location":"user_guide/#custom-models","title":"Custom models","text":"<p>You can also launch your own custom model as long as the model architecture is supported by vLLM, and make sure to follow the instructions below: * Your model weights directory naming convention should follow <code>$MODEL_FAMILY-$MODEL_VARIANT</code> ($MODEL_VARIANT is OPTIONAL). * Your model weights directory should contain HuggingFace format weights. * You should specify your model configuration by:   * Creating a custom configuration file for your model and specify its path via setting the environment variable <code>VEC_INF_CONFIG</code>. Check the default parameters file for the format of the config file. All the parameters for the model should be specified in that config file.   * Using launch command options to specify your model setup. * For other model launch parameters you can reference the default values for similar models using the <code>list</code> command .</p> <p>Here is an example to deploy a custom Qwen2.5-7B-Instruct-1M model which is not supported in the default list of models using a user custom config. In this case, the model weights are assumed to be downloaded to a <code>model-weights</code> directory inside the user's home directory. The weights directory of the model follows the naming convention so it would be named <code>Qwen2.5-7B-Instruct-1M</code>. The following yaml file would need to be created, lets say it is named <code>/h/&lt;username&gt;/my-model-config.yaml</code>.</p> <pre><code>models:\n  Qwen2.5-7B-Instruct-1M:\n    model_family: Qwen2.5\n    model_variant: 7B-Instruct-1M\n    model_type: LLM\n    gpus_per_node: 1\n    num_nodes: 1\n    vocab_size: 152064\n    qos: m2\n    time: 08:00:00\n    partition: a40\n    model_weights_parent_dir: /h/&lt;username&gt;/model-weights\n    vllm_args:\n      --max-model-len: 1010000\n      --max-num-seqs: 256\n</code></pre> <p>You would then set the <code>VEC_INF_CONFIG</code> path using:</p> <pre><code>export VEC_INF_CONFIG=/h/&lt;username&gt;/my-model-config.yaml\n</code></pre> <p>NOTE * There are other parameters that can also be added to the config but not shown in this example, check the <code>ModelConfig</code> for details. * Check vLLM Engine Arguments for the full list of available vLLM engine arguments. The default parallel size for any parallelization defaults to 1, so none of the sizes were set specifically in this example. * For GPU partitions with non-Ampere architectures, e.g. <code>rtx6000</code>, <code>t4v2</code>, BF16 isn't supported. For models that have BF16 as the default type, when using a non-Ampere GPU, use FP16 instead, i.e. <code>--dtype: float16</code>. * Setting <code>--compilation-config</code> to <code>3</code> currently breaks multi-node model launches, so we don't set them for models that require multiple nodes of GPUs.</p>"},{"location":"user_guide/#batch-launch-command","title":"<code>batch-launch</code> command","text":"<p>The <code>batch-launch</code> command allows users to launch multiple inference servers at once, here is an example of launching 2 models:</p> <pre><code>vec-inf batch-launch DeepSeek-R1-Distill-Qwen-7B Qwen2.5-Math-PRM-7B\n</code></pre> <p>You should see an output like the following:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Job Config     \u2503 Value                                                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Slurm Job ID   \u2502 17480109                                                                \u2502\n\u2502 Slurm Job Name \u2502 BATCH-DeepSeek-R1-Distill-Qwen-7B-Qwen2.5-Math-PRM-7B                   \u2502\n\u2502 Model Name     \u2502 DeepSeek-R1-Distill-Qwen-7B                                             \u2502\n\u2502 Partition      \u2502   a40                                                                   \u2502\n\u2502 QoS            \u2502   m2                                                                    \u2502\n\u2502 Time Limit     \u2502   08:00:00                                                              \u2502\n\u2502 Num Nodes      \u2502   1                                                                     \u2502\n\u2502 GPUs/Node      \u2502   1                                                                     \u2502\n\u2502 CPUs/Task      \u2502   16                                                                    \u2502\n\u2502 Memory/Node    \u2502   64G                                                                   \u2502\n\u2502 Log Directory  \u2502   /h/marshallw/.vec-inf-logs/BATCH-DeepSeek-R1-Distill-Qwen-7B-Qwen2.5\u2026 \u2502\n\u2502 Model Name     \u2502 Qwen2.5-Math-PRM-7B                                                     \u2502\n\u2502 Partition      \u2502   a40                                                                   \u2502\n\u2502 QoS            \u2502   m2                                                                    \u2502\n\u2502 Time Limit     \u2502   08:00:00                                                              \u2502\n\u2502 Num Nodes      \u2502   1                                                                     \u2502\n\u2502 GPUs/Node      \u2502   1                                                                     \u2502\n\u2502 CPUs/Task      \u2502   16                                                                    \u2502\n\u2502 Memory/Node    \u2502   64G                                                                   \u2502\n\u2502 Log Directory  \u2502   /h/marshallw/.vec-inf-logs/BATCH-DeepSeek-R1-Distill-Qwen-7B-Qwen2.5\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The inference servers will begin launching only after all requested resources have been allocated, preventing resource waste. Unlike the <code>launch</code> command, <code>batch-launch</code> does not accept additional launch parameters from the command line. Users must either:</p> <ul> <li>Specify a batch launch configuration file using the <code>--batch-config</code> option, or</li> <li>Ensure model launch configurations are available at the default location (cached config or user-defined <code>VEC_INF_CONFIG</code>)</li> </ul> <p>Since batch launches use heterogeneous jobs, users can request different partitions and resource amounts for each model. After launch, you can monitor individual servers using the standard commands (<code>status</code>, <code>metrics</code>, etc.) by providing the specific Slurm job ID for each server (e.g. 17480109+0, 17480109+1).</p> <p>NOTE * Currently only models that can fit on a single node (regardless of the node type) is supported, multi-node launches will be available in a future update.</p>"},{"location":"user_guide/#status-command","title":"<code>status</code> command","text":"<p>You can check the inference server status by providing the Slurm job ID to the <code>status</code> command:</p> <pre><code>vec-inf status 15373800\n</code></pre> <p>If the server is pending for resources, you should see an output like this:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Job Status     \u2503 Value                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Model Name     \u2502 Meta-Llama-3.1-8B-Instruct \u2502\n\u2502 Model Status   \u2502 PENDING                    \u2502\n\u2502 Pending Reason \u2502 Resources                  \u2502\n\u2502 Base URL       \u2502 UNAVAILABLE                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>When the server is ready, you should see an output like this:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Job Status   \u2503 Value                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Model Name   \u2502 Meta-Llama-3.1-8B-Instruct \u2502\n\u2502 Model Status \u2502 READY                      \u2502\n\u2502 Base URL     \u2502 http://gpu042:8080/v1      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>There are 5 possible states:</p> <ul> <li>PENDING: Job submitted to Slurm, but not executed yet. Job pending reason will be shown.</li> <li>LAUNCHING: Job is running but the server is not ready yet.</li> <li>READY: Inference server running and ready to take requests.</li> <li>FAILED: Inference server in an unhealthy state. Job failed reason will be shown.</li> <li>SHUTDOWN: Inference server is shutdown/cancelled.</li> </ul> <p>Note * The base URL is only available when model is in <code>READY</code> state. * For servers launched with <code>batch-launch</code>, the job ID should follow the format of \"MAIN_JOB_ID+OFFSET\" (e.g. 17480109+0, 17480109+1).</p>"},{"location":"user_guide/#metrics-command","title":"<code>metrics</code> command","text":"<p>Once your server is ready, you can check performance metrics by providing the Slurm job ID to the <code>metrics</code> command: <pre><code>vec-inf metrics 15373800\n</code></pre></p> <p>And you will see the performance metrics streamed to your console, note that the metrics are updated with a 2-second interval.</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Metric                  \u2503 Value           \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Prompt Throughput       \u2502 10.9 tokens/s   \u2502\n\u2502 Generation Throughput   \u2502 34.2 tokens/s   \u2502\n\u2502 Requests Running        \u2502 1 reqs          \u2502\n\u2502 Requests Waiting        \u2502 0 reqs          \u2502\n\u2502 Requests Swapped        \u2502 0 reqs          \u2502\n\u2502 GPU Cache Usage         \u2502 0.1%            \u2502\n\u2502 CPU Cache Usage         \u2502 0.0%            \u2502\n\u2502 Avg Request Latency     \u2502 2.6 s           \u2502\n\u2502 Total Prompt Tokens     \u2502 441 tokens      \u2502\n\u2502 Total Generation Tokens \u2502 1748 tokens     \u2502\n\u2502 Successful Requests     \u2502 14 reqs         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user_guide/#shutdown-command","title":"<code>shutdown</code> command","text":"<p>Finally, when you're finished using a model, you can shut it down by providing the Slurm job ID: <pre><code>vec-inf shutdown 15373800\n\n&gt; Shutting down model with Slurm Job ID: 15373800\n</code></pre></p>"},{"location":"user_guide/#list-command","title":"<code>list</code> command","text":"<p>You call view the full list of available models by running the <code>list</code> command:</p> <pre><code>vec-inf list\n</code></pre> <p></p> <p>You can also view the default setup for a specific supported model by providing the model name, for example <code>Meta-Llama-3.1-70B-Instruct</code>:</p> <pre><code>vec-inf list Meta-Llama-3.1-70B-Instruct\n</code></pre> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model Config             \u2503 Value                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 model_name               \u2502 Meta-Llama-3.1-8B-Instruct \u2502\n\u2502 model_family             \u2502 Meta-Llama-3.1             \u2502\n\u2502 model_variant            \u2502 8B-Instruct                \u2502\n\u2502 model_type               \u2502 LLM                        \u2502\n\u2502 gpus_per_node            \u2502 1                          \u2502\n\u2502 num_nodes                \u2502 1                          \u2502\n\u2502 cpus_per_task            \u2502 16                         \u2502\n\u2502 mem_per_node             \u2502 64G                        \u2502\n\u2502 vocab_size               \u2502 128256                     \u2502\n\u2502 qos                      \u2502 m2                         \u2502\n\u2502 time                     \u2502 08:00:00                   \u2502\n\u2502 partition                \u2502 a40                        \u2502\n\u2502 model_weights_parent_dir \u2502 /model-weights             \u2502\n\u2502 vLLM Arguments:          \u2502                            \u2502\n\u2502   --max-model-len:       \u2502 131072                     \u2502\n\u2502   --max-num-seqs:        \u2502 256                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p><code>launch</code>, <code>list</code>, and <code>status</code> command supports <code>--json-mode</code>, where the command output would be structured as a JSON string.</p>"},{"location":"user_guide/#check-job-configuration","title":"Check Job Configuration","text":"<p>With every model launch, a Slurm script will be generated dynamically based on the job and model configuration. Once the Slurm job is queued, the generated Slurm script will be moved to the log directory for reproducibility, located at <code>$log_dir/$model_family/$model_name.$slurm_job_id/$model_name.$slurm_job_id.slurm</code>. In the same directory you can also find a JSON file with the same name that captures the launch configuration, and will have an entry of server URL once the server is ready.</p>"},{"location":"user_guide/#send-inference-requests","title":"Send inference requests","text":"<p>Once the inference server is ready, you can start sending in inference requests. We provide example scripts for sending inference requests in <code>examples</code> folder. Make sure to update the model server URL and the model weights location in the scripts. For example, you can run <code>python examples/inference/llm/chat_completions.py</code>, and you should expect to see an output like the following:</p> <pre><code>{\n    \"id\":\"chatcmpl-387c2579231948ffaf66cdda5439d3dc\",\n    \"choices\": [\n        {\n            \"finish_reason\":\"stop\",\n            \"index\":0,\n            \"logprobs\":null,\n            \"message\": {\n                \"content\":\"Arrr, I be Captain Chatbeard, the scurviest chatbot on the seven seas! Ye be wantin' to know me identity, eh? Well, matey, I be a swashbucklin' AI, here to provide ye with answers and swappin' tales, savvy?\",\n                \"role\":\"assistant\",\n                \"function_call\":null,\n                \"tool_calls\":[],\n                \"reasoning_content\":null\n            },\n            \"stop_reason\":null\n        }\n    ],\n    \"created\":1742496683,\n    \"model\":\"Meta-Llama-3.1-8B-Instruct\",\n    \"object\":\"chat.completion\",\n    \"system_fingerprint\":null,\n    \"usage\": {\n        \"completion_tokens\":66,\n        \"prompt_tokens\":32,\n        \"total_tokens\":98,\n        \"prompt_tokens_details\":null\n    },\n    \"prompt_logprobs\":null\n}\n</code></pre> <p>NOTE: Certain models don't adhere to OpenAI's chat template, e.g. Mistral family. For these models, you can either change your prompt to follow the model's default chat template or provide your own chat template via <code>--chat-template: TEMPLATE_PATH</code>.</p>"},{"location":"user_guide/#ssh-tunnel-from-your-local-device","title":"SSH tunnel from your local device","text":"<p>If you want to run inference from your local device, you can open a SSH tunnel to your cluster environment like the following: <pre><code>ssh -L 8081:172.17.8.29:8081 username@v.vectorinstitute.ai -N\n</code></pre> Where the last number in the URL is the GPU number (gpu029 in this case). The example provided above is for the vector cluster, change the variables accordingly for your environment</p>"},{"location":"user_guide/#python-api-usage","title":"Python API Usage","text":"<p>You can also use the <code>vec_inf</code> Python API to launch and manage inference servers.</p> <p>Check out the Python API documentation for more details. There are also Python API usage examples in the <code>examples/api</code> folder.</p>"}]}